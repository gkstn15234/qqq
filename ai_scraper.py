import requests
from bs4 import BeautifulSoup
import xml.etree.ElementTree as ET
import re
import os
from datetime import datetime
import time
import random
import sys
import hashlib
import json
import base64
from urllib.parse import urlparse, urljoin
import sqlite3
from unidecode import unidecode

# AI ê´€ë ¨ import
try:
    from openai import OpenAI
    HAS_OPENAI = True
except ImportError:
    HAS_OPENAI = False

def get_env_var(name, default=None):
    """í™˜ê²½ë³€ìˆ˜ ê°€ì ¸ì˜¤ê¸°"""
    return os.environ.get(name, default)

def init_processed_db():
    """ì²˜ë¦¬ëœ ê¸°ì‚¬ ì¶”ì ì„ ìœ„í•œ SQLite DB ì´ˆê¸°í™”"""
    db_path = 'processed_articles.db'
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS processed_articles (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            url TEXT UNIQUE,
            title TEXT,
            hash TEXT,
            processed_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    
    conn.commit()
    conn.close()
    return db_path

def is_article_processed(url, title, article_hash):
    """ê¸°ì‚¬ê°€ ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆëŠ”ì§€ DBì—ì„œ í™•ì¸"""
    db_path = 'processed_articles.db'
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    
    # URL ë˜ëŠ” í•´ì‹œë¡œ í™•ì¸
    cursor.execute('''
        SELECT COUNT(*) FROM processed_articles 
        WHERE url = ? OR hash = ?
    ''', (url, article_hash))
    
    count = cursor.fetchone()[0]
    conn.close()
    
    return count > 0

def mark_article_processed(url, title, article_hash):
    """ê¸°ì‚¬ë¥¼ ì²˜ë¦¬ë¨ìœ¼ë¡œ DBì— ê¸°ë¡"""
    db_path = 'processed_articles.db'
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    
    try:
        cursor.execute('''
            INSERT OR REPLACE INTO processed_articles (url, title, hash)
            VALUES (?, ?, ?)
        ''', (url, title, article_hash))
        
        conn.commit()
    except Exception as e:
        print(f"âš ï¸ Failed to mark article as processed: {e}")
    finally:
        conn.close()

def clean_filename(title):
    """ì œëª©ì„ íŒŒì¼ëª…ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì •ë¦¬"""
    filename = re.sub(r'[^\w\s-]', '', title)
    filename = re.sub(r'[-\s]+', '-', filename)
    return filename.strip('-').lower()

def create_url_slug(title):
    """ì œëª©ì„ URL ìŠ¬ëŸ¬ê·¸ë¡œ ë³€í™˜ (ì˜ë¬¸)"""
    try:
        # í•œê¸€ì„ ì˜ë¬¸ìœ¼ë¡œ ë³€í™˜ (unidecode ì‚¬ìš©)
        slug = unidecode(title)
        # íŠ¹ìˆ˜ë¬¸ì ì œê±°, ê³µë°±ì„ í•˜ì´í”ˆìœ¼ë¡œ
        slug = re.sub(r'[^\w\s-]', '', slug)
        slug = re.sub(r'[-\s]+', '-', slug)
        # ì†Œë¬¸ìë¡œ ë³€í™˜, ì•ë’¤ í•˜ì´í”ˆ ì œê±°
        slug = slug.strip('-').lower()
        # ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸° (ìµœëŒ€ 60ì)
        if len(slug) > 60:
            slug = slug[:60].rstrip('-')
        return slug
    except:
        # unidecode ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ë°©ì‹ ì‚¬ìš©
        return clean_filename(title)

def categorize_article(title, content, tags):
    """ê¸°ì‚¬ë¥¼ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜"""
    title_lower = title.lower()
    content_lower = content.lower()
    all_tags = [tag.lower() for tag in tags]
    
    # ìë™ì°¨ ê´€ë ¨ í‚¤ì›Œë“œ
    car_keywords = [
        'car', 'auto', 'vehicle', 'ìë™ì°¨', 'ì°¨ëŸ‰', 'ìŠ¹ìš©ì°¨', 'íŠ¸ëŸ­', 'ë²„ìŠ¤',
        'í˜„ëŒ€', 'ê¸°ì•„', 'ì‚¼ì„±', 'í…ŒìŠ¬ë¼', 'tesla', 'hyundai', 'kia',
        'ì „ê¸°ì°¨', 'ev', 'electric', 'ìˆ˜ì†Œì°¨', 'hydrogen',
        'ì—”ì§„', 'ëª¨í„°', 'ë°°í„°ë¦¬', 'ì¶©ì „', 'ì£¼í–‰', 'ìš´ì „',
        'í´ë“œ', 'fold', 'ê°¤ëŸ­ì‹œ', 'galaxy', 'ìŠ¤ë§ˆíŠ¸í°', 'smartphone'
    ]
    
    # ê²½ì œ ê´€ë ¨ í‚¤ì›Œë“œ  
    economy_keywords = [
        'economy', 'economic', 'ê²½ì œ', 'ê¸ˆìœµ', 'íˆ¬ì', 'ì£¼ì‹', 'ì½”ìŠ¤í”¼', 'ì¦ì‹œ',
        'ë‹¬ëŸ¬', 'ì›í™”', 'í™˜ìœ¨', 'ê¸ˆë¦¬', 'ì¸í”Œë ˆì´ì…˜', 'ë¬¼ê°€',
        'ê¸°ì—…', 'íšŒì‚¬', 'ë§¤ì¶œ', 'ì´ìµ', 'ì†ì‹¤', 'ì‹¤ì ',
        'ì •ì±…', 'ì •ë¶€', 'ì€í–‰', 'ì¤‘ì•™ì€í–‰'
    ]
    
    # ê¸°ìˆ /IT ê´€ë ¨ í‚¤ì›Œë“œ
    tech_keywords = [
        'tech', 'technology', 'it', 'ê¸°ìˆ ', 'ì†Œí”„íŠ¸ì›¨ì–´', 'í•˜ë“œì›¨ì–´',
        'ai', 'ì¸ê³µì§€ëŠ¥', 'ë¨¸ì‹ ëŸ¬ë‹', 'ë”¥ëŸ¬ë‹', 
        'ì•±', 'app', 'í”Œë«í¼', 'platform', 'ì„œë¹„ìŠ¤',
        'êµ¬ê¸€', 'google', 'ì• í”Œ', 'apple', 'ë§ˆì´í¬ë¡œì†Œí”„íŠ¸', 'microsoft'
    ]
    
    # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
    car_score = sum(1 for keyword in car_keywords if keyword in title_lower or keyword in content_lower or keyword in all_tags)
    economy_score = sum(1 for keyword in economy_keywords if keyword in title_lower or keyword in content_lower or keyword in all_tags)
    tech_score = sum(1 for keyword in tech_keywords if keyword in title_lower or keyword in content_lower or keyword in all_tags)
    
    # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ ì¹´í…Œê³ ë¦¬ ì„ íƒ
    if car_score >= max(economy_score, tech_score):
        return 'automotive'
    elif economy_score >= tech_score:
        return 'economy'
    else:
        return 'technology'

def get_article_hash(title, url):
    """ê¸°ì‚¬ì˜ ê³ ìœ  í•´ì‹œ ìƒì„± (ì¤‘ë³µ ë°©ì§€ìš©)"""
    content = f"{title}{url}"
    return hashlib.md5(content.encode()).hexdigest()[:8]

def check_existing_articles(output_dir, article_hash, title, url):
    """ê°•í™”ëœ ê¸°ì‚¬ ì¤‘ë³µ ì²´í¬ (ì„œë¸Œë””ë ‰í† ë¦¬ í¬í•¨)"""
    if not os.path.exists(output_dir):
        return False
    
    # ì œëª© ê¸°ë°˜ ìœ ì‚¬ë„ ì²´í¬ë¥¼ ìœ„í•œ ì •ê·œí™”
    normalized_title = re.sub(r'[^\w\s]', '', title.lower()).strip()
    
    # ë£¨íŠ¸ ë””ë ‰í† ë¦¬ì™€ ëª¨ë“  ì„œë¸Œë””ë ‰í† ë¦¬ ê²€ì‚¬
    for root, dirs, files in os.walk(output_dir):
        for filename in files:
            if filename.endswith('.md'):
                filepath = os.path.join(root, filename)
                try:
                    with open(filepath, 'r', encoding='utf-8') as f:
                        content = f.read()
                        
                        # 1. í•´ì‹œ ê¸°ë°˜ ì²´í¬ (ê¸°ì¡´)
                        if f"hash: {article_hash}" in content:
                            return True
                        
                        # 2. URL ê¸°ë°˜ ì²´í¬ (ê°•í™”)
                        if f"source_url: \"{url}\"" in content:
                            return True
                        
                        # 3. ì œëª© ìœ ì‚¬ë„ ì²´í¬ (ì¶”ê°€)
                        title_match = re.search(r'title: "([^"]+)"', content)
                        if title_match:
                            existing_title = title_match.group(1)
                            existing_normalized = re.sub(r'[^\w\s]', '', existing_title.lower()).strip()
                            
                            # ì œëª©ì´ 90% ì´ìƒ ìœ ì‚¬í•˜ë©´ ì¤‘ë³µìœ¼ë¡œ íŒë‹¨
                            similarity = len(set(normalized_title.split()) & set(existing_normalized.split())) / max(len(normalized_title.split()), len(existing_normalized.split()), 1)
                            if similarity > 0.9:
                                return True
                                
                except Exception:
                    continue
    return False

def upload_to_cloudflare_images(image_url, api_token, account_id):
    """Cloudflare Imagesì— ì´ë¯¸ì§€ ì—…ë¡œë“œ"""
    try:
        # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        img_response = requests.get(image_url, headers=headers, timeout=10)
        img_response.raise_for_status()
        
        # Cloudflare Images API í˜¸ì¶œ
        upload_url = f"https://api.cloudflare.com/client/v4/accounts/{account_id}/images/v1"
        
        files = {
            'file': ('image.jpg', img_response.content, 'image/jpeg')
        }
        headers = {
            'Authorization': f'Bearer {api_token}'
        }
        
        response = requests.post(upload_url, files=files, headers=headers)
        response.raise_for_status()
        
        result = response.json()
        if result.get('success'):
            # Cloudflare Images URL ë°˜í™˜
            image_id = result['result']['id']
            return f"https://imagedelivery.net/{account_id}/{image_id}/public"
        else:
            print(f"âŒ Cloudflare upload failed: {result}")
            return image_url
            
    except Exception as e:
        print(f"âš ï¸ Failed to upload image to Cloudflare: {e}")
        return image_url

def rewrite_with_ai(original_content, title, api_key, api_type="openai"):
    """AIë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ì‚¬ ì¬ì‘ì„±"""
    if not api_key:
        print("âš ï¸ No AI API key provided, skipping rewrite")
        return original_content
    
    try:
        if api_type == "openai" and HAS_OPENAI:
            client = OpenAI(api_key=api_key)
            
            prompt = f"""
ë‹¤ìŒ ê¸°ì‚¬ë¥¼ ì™„ì „íˆ ìƒˆë¡œìš´ ê´€ì ì—ì„œ ì¬ì‘ì„±í•´ì£¼ì„¸ìš”. 
ì›ë³¸ì˜ í•µì‹¬ ì •ë³´ëŠ” ìœ ì§€í•˜ë˜, ë¬¸ì²´ì™€ êµ¬ì„±ì„ ì™„ì „íˆ ë°”ê¿”ì£¼ì„¸ìš”.
SEOì— ìµœì í™”ëœ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.

ì œëª©: {title}

ì›ë³¸ ê¸°ì‚¬:
{original_content}

ì¬ì‘ì„± ìš”êµ¬ì‚¬í•­:
1. ë¬¸ë‹¨ êµ¬ì„±ì„ ì™„ì „íˆ ìƒˆë¡­ê²Œ ë°°ì¹˜
2. í‘œí˜„ ë°©ì‹ì„ ë‹¤ë¥´ê²Œ ë³€ê²½
3. í•µì‹¬ ì‚¬ì‹¤ì€ ì •í™•íˆ ìœ ì§€
4. ìì—°ìŠ¤ëŸ½ê³  ì½ê¸° ì‰¬ìš´ ë¬¸ì²´
5. ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥
"""
            
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ì „ë¬¸ ê¸°ìì…ë‹ˆë‹¤. ê¸°ì‚¬ë¥¼ ìì—°ìŠ¤ëŸ½ê³  ë§¤ë ¥ì ìœ¼ë¡œ ì¬ì‘ì„±í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=2000,
                temperature=0.7
            )
            
            rewritten = response.choices[0].message.content.strip()
            return rewritten
            
    except Exception as e:
        print(f"âŒ AI rewrite failed: {e}")
        return original_content
    
    return original_content

def generate_ai_tags(title, content, existing_tags, api_key, api_type="openai"):
    """AIë¥¼ ì‚¬ìš©í•˜ì—¬ ì¶”ê°€ íƒœê·¸ ìƒì„±"""
    if not api_key:
        return existing_tags
    
    try:
        if api_type == "openai" and HAS_OPENAI:
            client = OpenAI(api_key=api_key)
            
            prompt = f"""
ë‹¤ìŒ ê¸°ì‚¬ì˜ ì œëª©ê³¼ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ íƒœê·¸ 2ê°œë¥¼ ì¶”ê°€ë¡œ ìƒì„±í•´ì£¼ì„¸ìš”.
ê¸°ì¡´ íƒœê·¸ì™€ ì¤‘ë³µë˜ì§€ ì•Šê²Œ í•˜ê³ , í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.

ì œëª©: {title}
ë‚´ìš©: {content[:500]}...
ê¸°ì¡´ íƒœê·¸: {', '.join(existing_tags)}

ìƒˆë¡œìš´ íƒœê·¸ 2ê°œë§Œ JSON ë°°ì—´ í˜•íƒœë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”.
ì˜ˆ: ["íƒœê·¸1", "íƒœê·¸2"]
"""
            
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ SEO ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê¸°ì‚¬ì— ë§ëŠ” ìµœì ì˜ íƒœê·¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=100,
                temperature=0.5
            )
            
            result = response.choices[0].message.content.strip()
            # JSON íŒŒì‹± ì‹œë„
            try:
                new_tags = json.loads(result)
                if isinstance(new_tags, list) and len(new_tags) >= 2:
                    return existing_tags + new_tags[:2]
            except:
                pass
                
    except Exception as e:
        print(f"âŒ AI tag generation failed: {e}")
    
    return existing_tags

def extract_content_from_url(url):
    """URLì—ì„œ ê¸°ì‚¬ ë‚´ìš© ì¶”ì¶œ (ìƒˆë¡œìš´ êµ¬ì¡° ëŒ€ì‘)"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ì œëª© ì¶”ì¶œ
        title_elem = soup.find('h1', class_='entry-title')
        if not title_elem:
            return None
        title = title_elem.get_text().strip()
        
        # ë©”íƒ€ ì •ë³´ ì¶”ì¶œ
        meta_elem = soup.find('div', class_='entry-meta')
        author = "ê¹€í•œìˆ˜"  # ê¸°ë³¸ê°’
        if meta_elem:
            author_elem = meta_elem.find('span', class_='author-name')
            if author_elem:
                author = author_elem.get_text().strip()
        
        # íƒœê·¸ ì¶”ì¶œ
        tags = ["ë‰´ìŠ¤", "ì´ìŠˆ"]  # ê¸°ë³¸ íƒœê·¸
        tags_section = soup.find('span', class_='tags-links')
        if tags_section:
            tag_links = tags_section.find_all('a', rel='tag')
            for tag_link in tag_links:
                tag_text = tag_link.get_text().strip()
                if tag_text not in tags:
                    tags.append(tag_text)
        
        # ë‚´ìš© ì¶”ì¶œ
        content_elem = soup.find('div', class_='entry-content')
        if not content_elem:
            return None
        
        # ê´‘ê³  ì œê±°
        for ad in content_elem.find_all('div', class_='repoad'):
            ad.decompose()
        for ad in content_elem.find_all('ins', class_='adsbygoogle'):
            ad.decompose()
        
        # ê³µìœ  ë²„íŠ¼ ì œê±°
        for share in content_elem.find_all('ul', class_='share-list'):
            share.decompose()
        
        # ì´ë¯¸ì§€ URL ìˆ˜ì§‘
        images = []
        for img in content_elem.find_all('img'):
            img_src = img.get('src')
            if img_src and ('wp-content/uploads' in img_src or 'reportera.b-cdn.net' in img_src):
                # ì ˆëŒ€ URLë¡œ ë³€í™˜
                if img_src.startswith('//'):
                    img_src = 'https:' + img_src
                elif img_src.startswith('/'):
                    img_src = 'https://www.reportera.co.kr' + img_src
                elif not img_src.startswith('http'):
                    img_src = 'https://www.reportera.co.kr/' + img_src
                images.append(img_src)
        
        # í…ìŠ¤íŠ¸ ë‚´ìš© ì¶”ì¶œ (ì´ë¯¸ì§€ ì œì™¸)
        paragraphs = []
        for elem in content_elem.children:
            if hasattr(elem, 'name') and elem.name:
                if elem.name in ['p', 'h1', 'h2', 'h3', 'h4', 'h5']:
                    # <br> íƒœê·¸ë¥¼ ì¤„ë°”ê¿ˆìœ¼ë¡œ ë³€í™˜
                    for br in elem.find_all('br'):
                        br.replace_with('\n')
                    
                    text = elem.get_text().strip()
                    if text and not text.startswith('(adsbygoogle'):
                        if elem.name in ['h2', 'h3', 'h4', 'h5']:
                            paragraphs.append(f"\n## {text}\n")
                        else:
                            paragraphs.append(text)
        
        content = '\n\n'.join(paragraphs)
        
        # ìš”ì•½ë¬¸ ìƒì„±
        description = paragraphs[0][:150] + "..." if paragraphs else ""
        
        return {
            'title': title,
            'description': description,
            'content': content,
            'images': images,
            'url': url,
            'author': author,
            'tags': tags
        }
    
    except Exception as e:
        print(f"âŒ Error extracting content from {url}: {e}")
        return None

def shuffle_images_in_content(content, cloudflare_images):
    """ì½˜í…ì¸  ë‚´ì— ì´ë¯¸ì§€ë¥¼ ëœë¤í•˜ê²Œ ì¬ë°°ì¹˜"""
    if not cloudflare_images:
        return content
    
    paragraphs = content.split('\n\n')
    
    # ì´ë¯¸ì§€ë¥¼ ëœë¤í•˜ê²Œ ì„ê¸°
    shuffled_images = cloudflare_images.copy()
    random.shuffle(shuffled_images)
    
    # ë¬¸ë‹¨ ì‚¬ì´ì— ì´ë¯¸ì§€ ì‚½ì…
    result_paragraphs = []
    image_index = 0
    
    for i, paragraph in enumerate(paragraphs):
        result_paragraphs.append(paragraph)
        
        # 2-3ê°œ ë¬¸ë‹¨ë§ˆë‹¤ ì´ë¯¸ì§€ ì‚½ì… (ëœë¤)
        if i > 0 and i % random.randint(2, 3) == 0 and image_index < len(shuffled_images):
            image_url = shuffled_images[image_index]
            result_paragraphs.append(f"\n![ê¸°ì‚¬ ì´ë¯¸ì§€]({image_url})\n")
            image_index += 1
    
    # ë‚¨ì€ ì´ë¯¸ì§€ë“¤ì„ ë§ˆì§€ë§‰ì— ì¶”ê°€
    while image_index < len(shuffled_images):
        image_url = shuffled_images[image_index]
        result_paragraphs.append(f"\n![ê¸°ì‚¬ ì´ë¯¸ì§€]({image_url})\n")
        image_index += 1
    
    return '\n\n'.join(result_paragraphs)

def create_markdown_file(article_data, output_dir, cloudflare_account_id=None, cloudflare_api_token=None, ai_api_key=None):
    """ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ìƒì„± (AI ì¬ì‘ì„± ë° ì´ë¯¸ì§€ ì²˜ë¦¬ í¬í•¨)"""
    # ë‹¤ë‹¨ê³„ ì¤‘ë³µ ì²´í¬
    article_hash = get_article_hash(article_data['title'], article_data['url'])
    
    # 1. DB ê¸°ë°˜ ì¤‘ë³µ ì²´í¬ (ë¹ ë¦„)
    if is_article_processed(article_data['url'], article_data['title'], article_hash):
        print(f"â­ï¸ Skipping duplicate article (DB): {article_data['title']}")
        return False
    
    # 2. íŒŒì¼ ê¸°ë°˜ ì¤‘ë³µ ì²´í¬ (ì•ˆì „ì¥ì¹˜)
    if check_existing_articles(output_dir, article_hash, article_data['title'], article_data['url']):
        print(f"â­ï¸ Skipping duplicate article (File): {article_data['title']}")
        # DBì—ë„ ê¸°ë¡
        mark_article_processed(article_data['url'], article_data['title'], article_hash)
        return False
    
    print(f"ğŸ¤– Processing with AI: {article_data['title'][:50]}...")
    
    # AIë¡œ ê¸°ì‚¬ ì¬ì‘ì„±
    rewritten_content = rewrite_with_ai(
        article_data['content'], 
        article_data['title'], 
        ai_api_key
    )
    
    # AIë¡œ íƒœê·¸ ì¶”ê°€ ìƒì„±
    enhanced_tags = generate_ai_tags(
        article_data['title'],
        article_data['content'],
        article_data['tags'],
        ai_api_key
    )
    
    # Cloudflareì— ì´ë¯¸ì§€ ì—…ë¡œë“œ
    cloudflare_images = []
    if cloudflare_api_token and cloudflare_account_id and article_data['images']:
        print(f"ğŸ“¸ Uploading {len(article_data['images'])} images to Cloudflare...")
        for img_url in article_data['images'][:5]:  # ìµœëŒ€ 5ê°œë§Œ
            cf_url = upload_to_cloudflare_images(img_url, cloudflare_api_token, cloudflare_account_id)
            cloudflare_images.append(cf_url)
            time.sleep(1)  # API ì œí•œ ê³ ë ¤
    
    # ì´ë¯¸ì§€ë¥¼ ì½˜í…ì¸ ì— ëœë¤ ì¬ë°°ì¹˜
    final_content = shuffle_images_in_content(rewritten_content, cloudflare_images)
    
    # ì¹´í…Œê³ ë¦¬ ìë™ ë¶„ë¥˜
    category = categorize_article(article_data['title'], article_data['content'], enhanced_tags)
    
    # URL ìŠ¬ëŸ¬ê·¸ ìƒì„± (ì˜ë¬¸)
    title_slug = create_url_slug(article_data['title'])
    
    # ì¹´í…Œê³ ë¦¬ë³„ ë””ë ‰í† ë¦¬ ìƒì„±
    category_dir = os.path.join(output_dir, category)
    os.makedirs(category_dir, exist_ok=True)
    
    # íŒŒì¼ëª… ìƒì„±: ì¹´í…Œê³ ë¦¬/ì œëª©-ì˜ë¬¸.md
    filename = f"{title_slug}.md"
    filepath = os.path.join(category_dir, filename)
    
    # íŒŒì¼ëª… ì¤‘ë³µ ë°©ì§€
    counter = 1
    while os.path.exists(filepath):
        filename = f"{title_slug}-{counter}.md"
        filepath = os.path.join(category_dir, filename)
        counter += 1
    
    # í˜„ì¬ ë‚ ì§œ
    current_date = datetime.now().strftime("%Y-%m-%dT%H:%M:%S+09:00")
    
    # ë§ˆí¬ë‹¤ìš´ ìƒì„±
    markdown_content = f"""---
title: "{article_data['title']}"
description: "{article_data['description']}"
date: {current_date}
author: "{article_data['author']}"
categories: ["{category}"]
tags: {json.dumps(enhanced_tags, ensure_ascii=False)}
hash: {article_hash}
source_url: "{article_data['url']}"
url: "/{category}/{title_slug}/"
"""
    
    # ì²« ë²ˆì§¸ ì´ë¯¸ì§€ë¥¼ ì¸ë„¤ì¼ë¡œ
    if cloudflare_images:
        markdown_content += f'images: ["{cloudflare_images[0]}"]\n'
    elif article_data['images']:
        markdown_content += f'images: ["{article_data["images"][0]}"]\n'
    
    markdown_content += f"""draft: false
---

{final_content}

---
*ì´ ê¸°ì‚¬ëŠ” AI ê¸°ìˆ ì„ í™œìš©í•˜ì—¬ ì¬ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.*
"""
    
    # íŒŒì¼ ì €ì¥
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(markdown_content)
    
    # DBì— ì²˜ë¦¬ ì™„ë£Œ ê¸°ë¡
    mark_article_processed(article_data['url'], article_data['title'], article_hash)
    
    print(f"âœ… Created: {os.path.basename(filepath)}")
    return True

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # í™˜ê²½ë³€ìˆ˜ì—ì„œ ì„¤ì • ì½ê¸°
    sitemap_url = get_env_var('SITEMAP_URL', 'https://www.reportera.co.kr/news-sitemap.xml')
    cloudflare_account_id = get_env_var('CLOUDFLARE_ACCOUNT_ID')
    cloudflare_api_token = get_env_var('CLOUDFLARE_API_TOKEN')
    ai_api_key = get_env_var('OPENAI_API_KEY')
    
    # ì²˜ë¦¬ëœ ê¸°ì‚¬ DB ì´ˆê¸°í™”
    init_processed_db()
    
    if len(sys.argv) > 1:
        sitemap_url = sys.argv[1]
    
    print(f"ğŸš€ Starting AI-powered scraper...")
    print(f"ğŸ“¥ Sitemap: {sitemap_url}")
    print(f"ğŸ¤– AI Rewrite: {'âœ…' if ai_api_key else 'âŒ'}")
    print(f"â˜ï¸ Cloudflare Images: {'âœ…' if cloudflare_api_token else 'âŒ'}")
    
    # ì‚¬ì´íŠ¸ë§µ ë‹¤ìš´ë¡œë“œ
    try:
        response = requests.get(sitemap_url)
        response.raise_for_status()
        sitemap_content = response.text
        print(f"âœ… Downloaded sitemap: {len(sitemap_content):,} bytes")
    except Exception as e:
        print(f"âŒ Error downloading sitemap: {e}")
        sys.exit(1)
    
    # URL ì¶”ì¶œ
    urls = []
    try:
        root = ET.fromstring(sitemap_content)
        # news sitemap ë„¤ì„ìŠ¤í˜ì´ìŠ¤
        namespaces = {
            '': 'http://www.sitemaps.org/schemas/sitemap/0.9',
            'news': 'http://www.google.com/schemas/sitemap-news/0.9'
        }
        
        for url_elem in root.findall('.//url', namespaces):
            loc_elem = url_elem.find('loc', namespaces)
            if loc_elem is not None:
                url = loc_elem.text
                if url and url.startswith('https://www.reportera.co.kr/'):
                    urls.append(url)
                    
    except Exception as e:
        print(f"âš ï¸ Error parsing XML: {e}")
        # ëŒ€ì•ˆ íŒŒì‹±
        lines = sitemap_content.split('\n')
        for line in lines:
            if '<loc>' in line and '</loc>' in line:
                start = line.find('<loc>') + 5
                end = line.find('</loc>')
                if start > 4 and end > start:
                    url = line[start:end]
                    if url.startswith('https://www.reportera.co.kr/'):
                        urls.append(url)
    
    # í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ 1ê°œ ê¸°ì‚¬ë§Œ ì²˜ë¦¬
    urls = urls[:1]
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬
    output_dir = 'content'
    os.makedirs(output_dir, exist_ok=True)
    
    print(f"ğŸ” Found {len(urls)} URLs to process")
    
    # ì²˜ë¦¬ í†µê³„
    processed = 0
    skipped = 0
    failed = 0
    
    for i, url in enumerate(urls):
        print(f"\nğŸ“„ [{i+1}/{len(urls)}] Processing: {url.split('/')[-1][:50]}...")
        
        article_data = extract_content_from_url(url)
        
        if article_data:
            if create_markdown_file(
                article_data, 
                output_dir,
                cloudflare_account_id,
                cloudflare_api_token,
                ai_api_key
            ):
                processed += 1
            else:
                skipped += 1
        else:
            failed += 1
        
        # API ì œí•œ ê³ ë ¤ ëŒ€ê¸°
        time.sleep(random.uniform(1, 2))
    
    print(f"\nğŸ“Š Processing Summary:")
    print(f"âœ… Processed: {processed}")
    print(f"â­ï¸ Skipped: {skipped}")
    print(f"âŒ Failed: {failed}")
    
    if processed > 0:
        print(f"ğŸ‰ Successfully created {processed} AI-rewritten articles!")
    else:
        print("â„¹ï¸ No new articles were created.")

if __name__ == "__main__":
    main() 