import requests
from bs4 import BeautifulSoup
import xml.etree.ElementTree as ET
import re
import os
from datetime import datetime, timezone, timedelta
import time
import random
import sys
import hashlib
import json
import base64
from urllib.parse import urlparse, urljoin
import sqlite3
from unidecode import unidecode

# AI ê´€ë ¨ import
try:
    from openai import OpenAI
    HAS_OPENAI = True
except ImportError:
    HAS_OPENAI = False

def get_env_var(name, default=None):
    """í™˜ê²½ë³€ìˆ˜ ê°€ì ¸ì˜¤ê¸°"""
    return os.environ.get(name, default)

def rebuild_hugo_site():
    """Hugo ì‚¬ì´íŠ¸ ì¬ë¹Œë“œ (ìƒˆ ê¸°ì‚¬ë¥¼ ë©”ì¸í˜ì´ì§€ì— ë°˜ì˜)"""
    try:
        import subprocess
        print("ğŸ”¨ Rebuilding Hugo site to reflect new articles...")
        
        # Hugo ë¹Œë“œ ëª…ë ¹ ì‹¤í–‰
        result = subprocess.run(
            ['hugo', '--gc', '--minify'], 
            capture_output=True, 
            text=True, 
            timeout=30,
            cwd=os.getcwd()
        )
        
        if result.returncode == 0:
            print("âœ… Hugo site rebuilt successfully!")
            return True
        else:
            print(f"âš ï¸ Hugo build warning: {result.stderr}")
            return False
            
    except subprocess.TimeoutExpired:
        print("âš ï¸ Hugo build timed out after 30 seconds")
        return False
    except FileNotFoundError:
        print("âš ï¸ Hugo not found - install Hugo or ensure it's in PATH")
        return False
    except Exception as e:
        print(f"âš ï¸ Hugo rebuild error: {e}")
        return False

def init_processed_db():
    """ì²˜ë¦¬ëœ ê¸°ì‚¬ ì¶”ì ì„ ìœ„í•œ SQLite DB ì´ˆê¸°í™”"""
    db_path = 'processed_articles.db'
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS processed_articles (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            url TEXT UNIQUE,
            title TEXT,
            hash TEXT,
            processed_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    
    conn.commit()
    conn.close()
    return db_path

def is_article_processed(url, title, article_hash):
    """ê¸°ì‚¬ê°€ ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆëŠ”ì§€ DBì—ì„œ í™•ì¸ (ê°•í™”ëœ URL ì²´í¬)"""
    db_path = 'processed_articles.db'
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    
    # 1. URL ì§ì ‘ ì²´í¬ (ê°€ì¥ í™•ì‹¤í•œ ë°©ë²•)
    cursor.execute('SELECT COUNT(*) FROM processed_articles WHERE url = ?', (url,))
    url_count = cursor.fetchone()[0]
    
    if url_count > 0:
        conn.close()
        return True
    
    # 2. í•´ì‹œ ê¸°ë°˜ ì²´í¬ (ì œëª©+URL ì¡°í•©)
    cursor.execute('SELECT COUNT(*) FROM processed_articles WHERE hash = ?', (article_hash,))
    hash_count = cursor.fetchone()[0]
    
    conn.close()
    return hash_count > 0

def mark_article_processed(url, title, article_hash):
    """ê¸°ì‚¬ë¥¼ ì²˜ë¦¬ë¨ìœ¼ë¡œ DBì— ê¸°ë¡"""
    db_path = 'processed_articles.db'
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    
    try:
        cursor.execute('''
            INSERT OR REPLACE INTO processed_articles (url, title, hash)
            VALUES (?, ?, ?)
        ''', (url, title, article_hash))
        
        conn.commit()
    except Exception as e:
        print(f"âš ï¸ Failed to mark article as processed: {e}")
    finally:
        conn.close()

def clean_filename(title):
    """ì œëª©ì„ íŒŒì¼ëª…ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì •ë¦¬"""
    filename = re.sub(r'[^\w\s-]', '', title)
    filename = re.sub(r'[-\s]+', '-', filename)
    return filename.strip('-').lower()

def create_url_slug(title):
    """ì œëª©ì„ URL ìŠ¬ëŸ¬ê·¸ë¡œ ë³€í™˜ (ì˜ë¬¸, 3~4ë‹¨ì–´ë¡œ ì œí•œ)"""
    try:
        # í•œê¸€ì„ ì˜ë¬¸ìœ¼ë¡œ ë³€í™˜ (unidecode ì‚¬ìš©)
        slug = unidecode(title)
        # íŠ¹ìˆ˜ë¬¸ì ì œê±°, ê³µë°±ì„ í•˜ì´í”ˆìœ¼ë¡œ
        slug = re.sub(r'[^\w\s-]', '', slug)
        slug = re.sub(r'[-\s]+', '-', slug)
        # ì†Œë¬¸ìë¡œ ë³€í™˜, ì•ë’¤ í•˜ì´í”ˆ ì œê±°
        slug = slug.strip('-').lower()
        
        # 3~4ë‹¨ì–´ë¡œ ì œí•œ (í•˜ì´í”ˆìœ¼ë¡œ êµ¬ë¶„ëœ ë‹¨ì–´ ê¸°ì¤€)
        words = slug.split('-')
        if len(words) > 4:
            # ì²« 4ê°œ ë‹¨ì–´ë§Œ ì‚¬ìš©
            slug = '-'.join(words[:4])
        elif len(words) < 3 and len(words) > 0:
            # 2ë‹¨ì–´ ì´í•˜ì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ìœ ì§€ (ë„ˆë¬´ ì§§ì§€ ì•Šë„ë¡)
            pass
        
        # ìµœëŒ€ ê¸¸ì´ ì œí•œ (ì•ˆì „ì¥ì¹˜)
        if len(slug) > 50:
            slug = slug[:50].rstrip('-')
            
        return slug
    except:
        # unidecode ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ë°©ì‹ ì‚¬ìš©
        return clean_filename(title)

def categorize_article(title, content, tags):
    """ê¸°ì‚¬ë¥¼ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜"""
    title_lower = title.lower()
    content_lower = content.lower()
    all_tags = [tag.lower() for tag in tags]
    
    # ìë™ì°¨ ê´€ë ¨ í‚¤ì›Œë“œ
    car_keywords = [
        'car', 'auto', 'vehicle', 'ìë™ì°¨', 'ì°¨ëŸ‰', 'ìŠ¹ìš©ì°¨', 'íŠ¸ëŸ­', 'ë²„ìŠ¤',
        'í˜„ëŒ€', 'ê¸°ì•„', 'ì‚¼ì„±', 'í…ŒìŠ¬ë¼', 'tesla', 'hyundai', 'kia',
        'ì „ê¸°ì°¨', 'ev', 'electric', 'ìˆ˜ì†Œì°¨', 'hydrogen',
        'ì—”ì§„', 'ëª¨í„°', 'ë°°í„°ë¦¬', 'ì¶©ì „', 'ì£¼í–‰', 'ìš´ì „',
        'í´ë“œ', 'fold', 'ê°¤ëŸ­ì‹œ', 'galaxy', 'ìŠ¤ë§ˆíŠ¸í°', 'smartphone'
    ]
    
    # ê²½ì œ ê´€ë ¨ í‚¤ì›Œë“œ  
    economy_keywords = [
        'economy', 'economic', 'ê²½ì œ', 'ê¸ˆìœµ', 'íˆ¬ì', 'ì£¼ì‹', 'ì½”ìŠ¤í”¼', 'ì¦ì‹œ',
        'ë‹¬ëŸ¬', 'ì›í™”', 'í™˜ìœ¨', 'ê¸ˆë¦¬', 'ì¸í”Œë ˆì´ì…˜', 'ë¬¼ê°€',
        'ê¸°ì—…', 'íšŒì‚¬', 'ë§¤ì¶œ', 'ì´ìµ', 'ì†ì‹¤', 'ì‹¤ì ',
        'ì •ì±…', 'ì •ë¶€', 'ì€í–‰', 'ì¤‘ì•™ì€í–‰'
    ]
    
    # ê¸°ìˆ /IT ê´€ë ¨ í‚¤ì›Œë“œ
    tech_keywords = [
        'tech', 'technology', 'it', 'ê¸°ìˆ ', 'ì†Œí”„íŠ¸ì›¨ì–´', 'í•˜ë“œì›¨ì–´',
        'ai', 'ì¸ê³µì§€ëŠ¥', 'ë¨¸ì‹ ëŸ¬ë‹', 'ë”¥ëŸ¬ë‹', 
        'ì•±', 'app', 'í”Œë«í¼', 'platform', 'ì„œë¹„ìŠ¤',
        'êµ¬ê¸€', 'google', 'ì• í”Œ', 'apple', 'ë§ˆì´í¬ë¡œì†Œí”„íŠ¸', 'microsoft'
    ]
    
    # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
    car_score = sum(1 for keyword in car_keywords if keyword in title_lower or keyword in content_lower or keyword in all_tags)
    economy_score = sum(1 for keyword in economy_keywords if keyword in title_lower or keyword in content_lower or keyword in all_tags)
    
    # automotive ë˜ëŠ” economy ì¹´í…Œê³ ë¦¬ë§Œ ì‚¬ìš©
    if car_score >= economy_score:
        return 'automotive'
    else:
        return 'economy'

def get_article_hash(title, url):
    """ê¸°ì‚¬ì˜ ê³ ìœ  í•´ì‹œ ìƒì„± (ì¤‘ë³µ ë°©ì§€ìš©)"""
    content = f"{title}{url}"
    return hashlib.md5(content.encode()).hexdigest()[:8]

def check_existing_articles(output_dir, article_hash, title, url):
    """ê°•í™”ëœ ê¸°ì‚¬ ì¤‘ë³µ ì²´í¬ (ì„œë¸Œë””ë ‰í† ë¦¬ í¬í•¨) - URL ìš°ì„ """
    if not os.path.exists(output_dir):
        return False
    
    # ì œëª© ê¸°ë°˜ ìœ ì‚¬ë„ ì²´í¬ë¥¼ ìœ„í•œ ì •ê·œí™”
    normalized_title = re.sub(r'[^\w\s]', '', title.lower()).strip()
    
    # ë£¨íŠ¸ ë””ë ‰í† ë¦¬ì™€ ëª¨ë“  ì„œë¸Œë””ë ‰í† ë¦¬ ê²€ì‚¬
    for root, dirs, files in os.walk(output_dir):
        for filename in files:
            if filename.endswith('.md'):
                filepath = os.path.join(root, filename)
                try:
                    with open(filepath, 'r', encoding='utf-8') as f:
                        content = f.read()
                        
                        # 1. URL ê¸°ë°˜ ì²´í¬ (ìµœìš°ì„  - ê°€ì¥ í™•ì‹¤)
                        if f'source_url: "{url}"' in content:
                            return True
                        
                        # 2. í•´ì‹œ ê¸°ë°˜ ì²´í¬
                        if f"hash: {article_hash}" in content:
                            return True
                        
                        # 3. ì œëª© ìœ ì‚¬ë„ ì²´í¬ (ë³´ì™„ì )
                        title_match = re.search(r'title: "([^"]+)"', content)
                        if title_match:
                            existing_title = title_match.group(1)
                            existing_normalized = re.sub(r'[^\w\s]', '', existing_title.lower()).strip()
                            
                            # ì œëª©ì´ 95% ì´ìƒ ìœ ì‚¬í•˜ë©´ ì¤‘ë³µìœ¼ë¡œ íŒë‹¨
                            if normalized_title and existing_normalized:
                                title_words = set(normalized_title.split())
                                existing_words = set(existing_normalized.split())
                                if title_words and existing_words:
                                    similarity = len(title_words & existing_words) / len(title_words | existing_words)
                                    if similarity > 0.95:
                                        return True
                                
                except Exception:
                    continue
    return False

def create_manual_rewrite(original_content, title):
    """AI ì‹¤íŒ¨ ì‹œ ìˆ˜ë™ìœ¼ë¡œ ê¸°ì‚¬ ì¬ì‘ì„± - ê·¹ë‹¨ì  ë³€í˜•"""
    try:
        # ì›ë³¸ ì½˜í…ì¸ ë¥¼ ë¬¸ë‹¨ë³„ë¡œ ë¶„ë¦¬
        paragraphs = original_content.split('\n\n')
        rewritten_paragraphs = []
        
        # ë¬¸ì²´ ë³€í˜•ì„ ìœ„í•œ í‘œí˜„ ì‚¬ì „
        style_transforms = {
            "ë°œí‘œí–ˆë‹¤": ["ê³µê°œí–ˆë‹¤", "ë°í˜”ë‹¤", "ì•Œë ¸ë‹¤", "ì „í–ˆë‹¤", "ê³µí‘œí–ˆë‹¤"],
            "ì¦ê°€í–ˆë‹¤": ["ëŠ˜ì–´ë‚¬ë‹¤", "ìƒìŠ¹í–ˆë‹¤", "í™•ëŒ€ëë‹¤", "ì„±ì¥í–ˆë‹¤", "ì˜¤ë¦„ì„¸ë¥¼ ë³´ì˜€ë‹¤"],
            "ê°ì†Œí–ˆë‹¤": ["ì¤„ì–´ë“¤ì—ˆë‹¤", "í•˜ë½í–ˆë‹¤", "ì¶•ì†Œëë‹¤", "ë‚´ë¦¼ì„¸ë¥¼ ë³´ì˜€ë‹¤", "ë‘”í™”ëë‹¤"],
            "ê³„íšì´ë‹¤": ["ì˜ˆì •ì´ë‹¤", "ë°©ì¹¨ì´ë‹¤", "êµ¬ìƒì´ë‹¤", "ì˜ë„ë‹¤", "ê³„íšì„ ì„¸ì› ë‹¤"],
            "ë¬¸ì œê°€": ["ì´ìŠˆê°€", "ìš°ë ¤ê°€", "ìŸì ì´", "ê³¼ì œê°€", "ë‚œì œê°€"],
            "ì¤‘ìš”í•˜ë‹¤": ["í•µì‹¬ì ì´ë‹¤", "ì£¼ìš”í•˜ë‹¤", "ê²°ì •ì ì´ë‹¤", "í•„ìˆ˜ì ì´ë‹¤", "ê´€ê±´ì´ë‹¤"],
            "ì§„í–‰ëë‹¤": ["ì´ë¤„ì¡Œë‹¤", "ì¶”ì§„ëë‹¤", "ì‹¤ì‹œëë‹¤", "ê°œìµœëë‹¤", "í¼ì³ì¡Œë‹¤"]
        }
        
        # ì ‘ì†ì‚¬ ë° ì‹œì‘ í‘œí˜„ ë‹¤ì–‘í™”
        connectors = [
            "í•œí¸", "ë˜í•œ", "ì´ì™€ ê´€ë ¨í•´", "íŠ¹íˆ", "ë”ë¶ˆì–´", "ì•„ìš¸ëŸ¬", 
            "ê·¸ëŸ° ê°€ìš´ë°", "ì´ëŸ° ìƒí™©ì—ì„œ", "ì£¼ëª©í•  ì ì€", "ëˆˆì—¬ê²¨ë³¼ ëŒ€ëª©ì€",
            "ì—…ê³„ì— ë”°ë¥´ë©´", "ì „ë¬¸ê°€ë“¤ì€", "ê´€ê³„ìë“¤ì— ì˜í•˜ë©´"
        ]
        
        # ê° ë¬¸ë‹¨ì„ ê·¹ë‹¨ì ìœ¼ë¡œ ì¬êµ¬ì„±
        for i, paragraph in enumerate(paragraphs):
            if not paragraph.strip():
                continue
                
            sentences = paragraph.split('.')
            if len(sentences) > 1:
                rewritten_sentences = []
                
                for j, sentence in enumerate(sentences):
                    sentence = sentence.strip()
                    if not sentence:
                        continue
                    
                    # 1. í‘œí˜„ ì‚¬ì „ì„ í™œìš©í•œ ì–´íœ˜ ë³€ê²½
                    for original, alternatives in style_transforms.items():
                        if original in sentence:
                            import random
                            sentence = sentence.replace(original, random.choice(alternatives))
                    
                    # 2. ë¬¸ì¥ êµ¬ì¡° ë³€í˜•
                    if "ëŠ”" in sentence and "ì´ë‹¤" in sentence:
                        # "AëŠ” Bì´ë‹¤" â†’ "Bë¡œ ë‚˜íƒ€ë‚˜ëŠ” ê²ƒì´ Aë‹¤"
                        parts = sentence.split("ëŠ”")
                        if len(parts) == 2:
                            subject = parts[0].strip()
                            predicate = parts[1].strip()
                            if "ì´ë‹¤" in predicate:
                                predicate = predicate.replace("ì´ë‹¤", "ë¡œ í™•ì¸ë˜ëŠ” ê²ƒì´")
                                sentence = f"{predicate} {subject}ë‹¤"
                    
                    # 3. ìˆ«ì í‘œí˜„ ë³€í˜•
                    import re
                    percent_pattern = r'(\d+)%'
                    sentence = re.sub(percent_pattern, lambda m: f"100ëª… ì¤‘ {m.group(1)}ëª…", sentence)
                    
                    # 4. ë¬¸ì¥ ì‹œì‘ ë‹¤ì–‘í™”
                    if j == 0 and i > 0:
                        connector = connectors[i % len(connectors)]
                        if not any(sentence.startswith(conn) for conn in connectors):
                            sentence = f"{connector} {sentence.lower()}"
                    
                    # 5. ì§ˆë¬¸í˜•/ê°íƒ„í˜• ë³€í˜• (ì¼ë¶€ ë¬¸ì¥ì„)
                    if j % 3 == 0 and "ì¤‘ìš”" in sentence:
                        sentence = sentence.replace("ì¤‘ìš”í•˜ë‹¤", "ì¤‘ìš”í•˜ì§€ ì•Šì„ê¹Œ?")
                    elif "ë†€ë¼ìš´" in sentence or "ì£¼ëª©" in sentence:
                        sentence = sentence + "!"
                    
                    rewritten_sentences.append(sentence)
                
                if rewritten_sentences:
                    # ë¬¸ì¥ ìˆœì„œë„ ì¼ë¶€ ë³€ê²½
                    if len(rewritten_sentences) > 2:
                        # ë§ˆì§€ë§‰ ë¬¸ì¥ì„ ì•ìœ¼ë¡œ ì´ë™ (ë•Œë•Œë¡œ)
                        if i % 2 == 0:
                            last_sentence = rewritten_sentences.pop()
                            rewritten_sentences.insert(0, last_sentence)
                    
                    rewritten_paragraphs.append('. '.join(rewritten_sentences) + '.')
            else:
                # ë‹¨ì¼ ë¬¸ì¥ë„ ë³€í˜•
                paragraph = paragraph.strip()
                for original, alternatives in style_transforms.items():
                    if original in paragraph:
                        import random
                        paragraph = paragraph.replace(original, random.choice(alternatives))
                rewritten_paragraphs.append(paragraph)
        
        # 35~60ëŒ€ ë…ìì¸µì„ ìœ„í•œ ê¸°ë³¸ êµ¬ì¡°ë¡œ ì¬êµ¬ì„± (H5 í•˜ë‚˜ì— <br> ë‘ ì¤„ + ì¸ë„¤ì¼ + ë³¸ë¬¸ + H2 ì†Œì œëª©)
        rewritten_content = f"""##### **{title}ì˜ í•µì‹¬ ë‚´ìš© ìš”ì•½**<br>**ì—…ê³„ ë™í–¥ê³¼ í–¥í›„ ì „ë§ ë¶„ì„**

{chr(10).join(rewritten_paragraphs[:3])}

## í•µì‹¬ í¬ì¸íŠ¸

{chr(10).join(rewritten_paragraphs[3:6]) if len(rewritten_paragraphs) > 3 else ''}

## ìƒì„¸ ë¶„ì„

{chr(10).join(rewritten_paragraphs[6:]) if len(rewritten_paragraphs) > 6 else ''}

**ì´ë²ˆ ì´ìŠˆëŠ” ì—…ê³„ì— ì¤‘ìš”í•œ ì‹œì‚¬ì ì„ ì œê³µí•˜ê³  ìˆìœ¼ë©°**, í–¥í›„ ë™í–¥ì— ëŒ€í•œ ì§€ì†ì ì¸ ê´€ì‹¬ì´ í•„ìš”í•´ ë³´ì…ë‹ˆë‹¤.
"""
        
        return rewritten_content.strip()
        
    except Exception as e:
        print(f"âš ï¸ Manual rewrite failed: {e}")
        # ìµœì†Œí•œì˜ ê¸°ë³¸ êµ¬ì¡°ë¼ë„ ìƒì„± (H5 í•˜ë‚˜ì— <br> ë‘ ì¤„ + H2 ì†Œì œëª©)
        return f"""##### **ì—…ê³„ ì£¼ìš” ë™í–¥ í•µì‹¬ ë¶„ì„**<br>**{title} ì˜í–¥ê³¼ ì‹œì¥ ì „ë§**

ë³¸ ê¸°ì‚¬ëŠ” í˜„ì¬ ì—…ê³„ì˜ ì£¼ìš” ë™í–¥ì„ ë‹¤ë£¨ê³  ìˆìŠµë‹ˆë‹¤.

## í•µì‹¬ í¬ì¸íŠ¸

ê´€ë ¨ ì—…ê³„ì—ì„œëŠ” ì´ë²ˆ ì‚¬ì•ˆì— ëŒ€í•´ **ë†’ì€ ê´€ì‹¬ì„ ë³´ì´ê³  ìˆìœ¼ë©°**, ë‹¤ì–‘í•œ ì˜ê²¬ì´ ì œê¸°ë˜ê³  ìˆëŠ” ìƒí™©ì…ë‹ˆë‹¤.

## í–¥í›„ ì „ë§

ì´ëŸ¬í•œ ë³€í™”ëŠ” ì‹œì¥ì— ì¤‘ëŒ€í•œ ì˜í–¥ì„ ë¯¸ì¹  ê²ƒìœ¼ë¡œ ì˜ˆìƒë˜ë©°, **ê´€ë ¨ ê¸°ì—…ë“¤ì˜ ëŒ€ì‘ ì „ëµì´ ì£¼ëª©ë°›ê³  ìˆìŠµë‹ˆë‹¤**.

*ë³¸ ê¸°ì‚¬ëŠ” ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.*
"""

def upload_to_cloudflare_images(image_url, api_token, account_id):
    """Cloudflare Imagesì— ì´ë¯¸ì§€ ì—…ë¡œë“œ"""
    try:
        # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        img_response = requests.get(image_url, headers=headers, timeout=10)
        img_response.raise_for_status()
        
        # Cloudflare Images API í˜¸ì¶œ
        upload_url = f"https://api.cloudflare.com/client/v4/accounts/{account_id}/images/v1"
        
        files = {
            'file': ('image.jpg', img_response.content, 'image/jpeg')
        }
        headers = {
            'Authorization': f'Bearer {api_token}'
        }
        
        response = requests.post(upload_url, files=files, headers=headers)
        response.raise_for_status()
        
        result = response.json()
        if result.get('success'):
            # Cloudflare Images URL ë°˜í™˜ (ìƒˆë¡œìš´ account hash ì‚¬ìš©)
            image_id = result['result']['id']
            account_hash = "BhPWbivJAhTvor9c-8lV2w"  # ìƒˆë¡œìš´ account hash
            cloudflare_url = f"https://imagedelivery.net/{account_hash}/{image_id}/public"
            print(f"ğŸ“¸ Cloudflare image URL: {cloudflare_url}")
            return cloudflare_url
        else:
            print(f"âŒ Cloudflare upload failed: {result}")
            return None  # ì‹¤íŒ¨ ì‹œ None ë°˜í™˜
            
    except Exception as e:
        print(f"âš ï¸ Failed to upload image to Cloudflare: {e}")
        return None  # ì‹¤íŒ¨ ì‹œ None ë°˜í™˜

def rewrite_with_ai(original_content, title, api_key, api_type="openai"):
    """AIë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ì‚¬ ì¬ì‘ì„±"""
    if not api_key:
        raise Exception("No AI API key provided - AI rewrite is mandatory")
    
    # ìµœëŒ€ 3ë²ˆ ì¬ì‹œë„
    for attempt in range(3):
        try:
            print(f"ğŸ¤– AI rewrite attempt {attempt + 1}/3...")
            if api_type == "openai" and HAS_OPENAI:
                client = OpenAI(api_key=api_key)
                
                prompt = f"""
ë‹¤ìŒ ì›ë³¸ ê¸°ì‚¬ë¥¼ ë¶„ì„í•˜ì—¬ **ì™„ì „íˆ ìƒˆë¡œìš´ ê´€ì ê³¼ ë¬¸ì²´**ë¡œ ì¬ì°½ì‘í•´ì£¼ì„¸ìš”.
ì›ë³¸ ì‘ì„±ìê°€ ìì‹ ì˜ ê¸€ì´ë¼ê³  ì¸ì‹í•  ìˆ˜ ì—†ì„ ì •ë„ë¡œ **í˜ì‹ ì ìœ¼ë¡œ ë³€í˜•**í•´ì£¼ì„¸ìš”.

ì œëª©: {title}

ì›ë³¸ ê¸°ì‚¬:
{original_content}

**ê·¹ë‹¨ì  ë³€í˜• ìš”êµ¬ì‚¬í•­:**
1. **ë¬¸ì²´ ì™„ì „ ë³€ê²½**: ì›ë³¸ì´ ë”±ë”±í•˜ë©´ ì¹œê·¼í•˜ê²Œ, ì¹œê·¼í•˜ë©´ ì „ë¬¸ì ìœ¼ë¡œ ë°”ê¿”ì£¼ì„¸ìš”
2. **ì‹œì‘ ê°ë„ í˜ì‹ **: ì›ë³¸ê³¼ ì „í˜€ ë‹¤ë¥¸ ê´€ì ì—ì„œ ì‚¬ê±´ì„ ì ‘ê·¼í•´ì£¼ì„¸ìš”
3. **ë¬¸ì¥ êµ¬ì¡° íŒŒê´´**: ì›ë³¸ì˜ ë¬¸ì¥ íŒ¨í„´ì„ ì™„ì „íˆ í•´ì²´í•˜ê³  ì¬êµ¬ì„±í•´ì£¼ì„¸ìš”
4. **ì–´íœ˜ ì„ íƒ ë³€í™”**: ê°™ì€ ì˜ë¯¸ì˜ ë‹¤ë¥¸ í‘œí˜„, ë‹¤ë¥¸ ë‰˜ì•™ìŠ¤ë¡œ ë°”ê¿”ì£¼ì„¸ìš”
5. **ë…¼ë¦¬ íë¦„ ì¬ë°°ì¹˜**: ì •ë³´ ì œì‹œ ìˆœì„œë¥¼ ì™„ì „íˆ ì¬ë°°ì—´í•´ì£¼ì„¸ìš”
6. **ìŠ¤íƒ€ì¼ ì •ì²´ì„± ë³€ê²½**: ë§ˆì¹˜ ì„±ê²©ì´ ë‹¤ë¥¸ ê¸°ìê°€ ì“´ ê²ƒì²˜ëŸ¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”
7. **í‘œí˜„ ê¸°ë²• ë‹¤ë³€í™”**: 
   - ì§ˆë¬¸í˜•/ì„œìˆ í˜•/ê°íƒ„í˜•ì„ ë‹¤ì–‘í•˜ê²Œ í™œìš©
   - ë¹„ìœ ì™€ ì€ìœ  í‘œí˜„ ì¶”ê°€
   - ìˆ«ì í‘œí˜„ ë°©ì‹ ë³€ê²½ (ì˜ˆ: "30%" â†’ "10ëª… ì¤‘ 3ëª…")
8. **ê°ì • í†¤ ë³€ê²½**: ì›ë³¸ì˜ ê°ì •ì  í†¤ì„ ì™„ì „íˆ ë‹¤ë¥´ê²Œ ì„¤ì •
9. **ë…ì ê´€ì  ì „í™˜**: ë‹¤ë¥¸ ë…ìì¸µì—ê²Œ ë§í•˜ëŠ” ê²ƒì²˜ëŸ¼ í†¤ì•¤ë§¤ë„ˆ ë³€ê²½
10. **í•µì‹¬ ì‚¬ì‹¤ë§Œ ë³´ì¡´**: ë‚ ì§œ, ìˆ˜ì¹˜, ê³ ìœ ëª…ì‚¬, í•µì‹¬ ì‚¬ì‹¤ì€ ì •í™•íˆ ìœ ì§€

**ì‹œì¸ì„± í–¥ìƒì„ ìœ„í•œ êµµê²Œ ì²˜ë¦¬ (ìµœìš°ì„ ):**
- **í•µì‹¬ í‚¤ì›Œë“œì™€ ì¤‘ìš” ì •ë³´**ë¥¼ ë°˜ë“œì‹œ **êµµê²Œ** í‘œì‹œ
- **ìˆ˜ì¹˜, ë‚ ì§œ, ê¸°ì—…ëª…, ì œí’ˆëª…** ë“±ì€ ëª¨ë‘ **êµµê²Œ** ì²˜ë¦¬
- **ì£¼ìš” ë³€í™”ë‚˜ ê²°ë¡ **ì€ **êµµê²Œ** ê°•ì¡°
- **ë…ìê°€ ê¼­ ê¸°ì–µí•´ì•¼ í•  ë‚´ìš©**ì€ **êµµê²Œ** í‘œì‹œ
- ë¬¸ë‹¨ë§ˆë‹¤ ìµœì†Œ 2-3ê°œì˜ **êµµì€ í‚¤ì›Œë“œ** í¬í•¨
- **35-60ëŒ€ ë…ìì¸µ**ì´ í•µì‹¬ë§Œ ë¹ ë¥´ê²Œ íŒŒì•…í•  ìˆ˜ ìˆë„ë¡ **êµµê²Œ** í™œìš©

**ë¬¸ì²´ ë³€í˜• ì˜ˆì‹œ:**
- ì›ë³¸: "íšŒì‚¬ê°€ ë°œí‘œí–ˆë‹¤" â†’ ë³€í˜•: "**ì—…ì²´ ì¸¡ì´ ê³µê°œí•œ** ë°”ì— ë”°ë¥´ë©´"
- ì›ë³¸: "ì¦ê°€í–ˆë‹¤" â†’ ë³€í˜•: "**ìƒìŠ¹ì„¸ë¥¼ ë³´ì´ê³ ** ìˆë‹¤", "**ëŠ˜ì–´ë‚˜ëŠ” ì¶”ì„¸**ë‹¤"
- ì›ë³¸: "ë¬¸ì œê°€ ìˆë‹¤" â†’ ë³€í˜•: "**ìš°ë ¤ìŠ¤ëŸ¬ìš´ ìƒí™©**ì´ ë²Œì–´ì§€ê³  ìˆë‹¤"

**í—¤ë”© êµ¬ì¡° (ì ˆëŒ€ ì—„ìˆ˜):**
##### [ì²« ë²ˆì§¸ ì¤„ ìš”ì•½]<br>[ë‘ ë²ˆì§¸ ì¤„ ìš”ì•½]

**í—¤ë”© ì‚¬ìš© ê·œì¹™:**
- H5(#####): í•˜ë‚˜ì˜ íƒœê·¸ ì•ˆì— <br>ë¡œ ë‘ ì¤„ ì‘ì„± (| ì‘ëŒ€ê¸° ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
- H2(##): ëª¨ë“  ì†Œì œëª©ì— ì‚¬ìš© (H3, H4, H6 ì ˆëŒ€ ê¸ˆì§€!)
- H1(#): ì‚¬ìš© ê¸ˆì§€ (Hugoì—ì„œ ìë™ ìƒì„±)

**H2 ì†Œì œëª© ì‘ì„± ê·œì¹™:**
- ì½œë¡ (:), ëŠë‚Œí‘œ(!), ë¬¼ìŒí‘œ(?) ë“± íŠ¹ìˆ˜ê¸°í˜¸ ì‚¬ìš© ê¸ˆì§€
- ìì—°ìŠ¤ëŸ¬ìš´ ëª…ì‚¬í˜• ë˜ëŠ” ì„œìˆ í˜•ìœ¼ë¡œ ì‘ì„±
- ì˜ˆì‹œ: "ì£¼ìš” ë³€í™” ë™í–¥", "ì‹œì¥ ë°˜ì‘ê³¼ ì „ë§", "ì—…ê³„ ë¶„ì„ ê²°ê³¼"

**ê¸°ì‚¬ êµ¬ì¡° (ì ˆëŒ€ ì¤€ìˆ˜):**
1. H5 ìš”ì•½: ##### **ì²« ë²ˆì§¸ ì¤„**<br>**ë‘ ë²ˆì§¸ ì¤„**
2. ë„ì… ë³¸ë¬¸: 2-3ê°œ ë¬¸ë‹¨ (H2 ì—†ì´ ë°”ë¡œ ë³¸ë¬¸ìœ¼ë¡œ ì‹œì‘, **êµµì€ í‚¤ì›Œë“œ** í¬í•¨)
3. H2 ì†Œì œëª© + ë³¸ë¬¸ ë°˜ë³µ (ê° ë¬¸ë‹¨ë§ˆë‹¤ **í•µì‹¬ ì •ë³´ êµµê²Œ** ì²˜ë¦¬)

**H5 ìš”ì•½ í•„ìˆ˜ í˜•ì‹:**
##### **500ë§ˆë ¥ ì „ê¸° SUV êµ­ë‚´ ìƒë¥™ ì˜ˆê³ **<br>**ëŸ­ì…”ë¦¬ì™€ ì˜¤í”„ë¡œë“œ ëŠ¥ë ¥ ëª¨ë‘ ê°–ì¶°**

**ê¸°ì‚¬ ì‹œì‘ êµ¬ì¡° ì˜ˆì‹œ:**
##### **í•µì‹¬ ë‚´ìš© ìš”ì•½**<br>**ë¶€ê°€ ì„¤ëª… ìš”ì•½**

ì—…ê³„ì—ì„œëŠ” ì´ë²ˆ ë°œí‘œê°€ **ì‹œì¥ì— í° ë³€í™”**ë¥¼ ê°€ì ¸ì˜¬ ê²ƒìœ¼ë¡œ ì „ë§í•˜ê³  ìˆë‹¤. 

ê´€ë ¨ ì „ë¬¸ê°€ë“¤ì€ ì´ëŸ¬í•œ ì›€ì§ì„ì´ í–¥í›„ **ì—…ê³„ ì „ë°˜ì— ë¯¸ì¹  íŒŒê¸‰íš¨ê³¼**ë¥¼ ì£¼ëª©í•˜ê³  ìˆìœ¼ë©°, ë‹¤ì–‘í•œ ë¶„ì„ì´ ì œê¸°ë˜ê³  ìˆëŠ” ìƒí™©ì´ë‹¤.

íŠ¹íˆ ì´ë²ˆ ì‚¬ì•ˆì€ **ê¸°ì¡´ ì‹œì¥ êµ¬ì¡°ì— ìƒˆë¡œìš´ ë³€ìˆ˜**ë¡œ ì‘ìš©í•  ê²ƒìœ¼ë¡œ ì˜ˆìƒë˜ë©°, ê´€ë ¨ ê¸°ì—…ë“¤ì˜ **ëŒ€ì‘ ì „ëµ**ì—ë„ ê´€ì‹¬ì´ ì§‘ì¤‘ë˜ê³  ìˆë‹¤.

## ì£¼ìš” ë³€í™” ë™í–¥

(ì´í›„ H2 + ë³¸ë¬¸ ë°˜ë³µ, ëª¨ë“  ë¬¸ë‹¨ì— **ì¤‘ìš” í‚¤ì›Œë“œ êµµê²Œ** ì²˜ë¦¬...)

**ìµœì¢… ëª©í‘œ: ì›ë³¸ ì‘ì„±ìê°€ "ì´ê±´ ë‚´ ê¸€ì´ ì•„ë‹ˆì•¼!"ë¼ê³  í•  ì •ë„ë¡œ ì™„ì „íˆ ë‹¤ë¥¸ ì‘í’ˆì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”.**
ê°™ì€ ì‚¬ê±´ì„ ë‹¤ë£¬ ì „í˜€ ë‹¤ë¥¸ ê¸°ìì˜ ë…ë¦½ì ì¸ ì·¨ì¬ ê¸°ì‚¬ì²˜ëŸ¼ ì‘ì„±í•´ì£¼ë˜, **í•µì‹¬ ì •ë³´ëŠ” êµµê²Œ ê°•ì¡°**í•˜ì—¬ 35-60ëŒ€ ë…ìì¸µì´ **ë¹ ë¥´ê²Œ í•µì‹¬ì„ íŒŒì•…**í•  ìˆ˜ ìˆë„ë¡ í•´ì£¼ì„¸ìš”.
"""
                
                response = client.chat.completions.create(
                    model="gpt-4.1",  # gpt-4o-mini â†’ gpt-4.1ë¡œ ë³€ê²½
                    messages=[
                        {"role": "system", "content": "ë‹¹ì‹ ì€ ì°½ì‘ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ì™„ì „íˆ ìƒˆë¡œìš´ ìŠ¤íƒ€ì¼ë¡œ ë³€í˜•í•˜ì—¬ ì›ì €ì‘ìë„ ì¸ì‹í•  ìˆ˜ ì—†ê²Œ ë§Œë“œëŠ” ì¬ì°½ì‘ì˜ ë‹¬ì¸ì…ë‹ˆë‹¤. ê°™ì€ ì‚¬ì‹¤ì„ ì „í˜€ ë‹¤ë¥¸ í‘œí˜„ê³¼ êµ¬ì¡°ë¡œ ì¬íƒ„ìƒì‹œí‚¤ëŠ” ê²ƒì´ ë‹¹ì‹ ì˜ íŠ¹ê¸°ì…ë‹ˆë‹¤. ë¬¸ì²´, í†¤, êµ¬ì¡°, í‘œí˜„ì„ í˜ì‹ ì ìœ¼ë¡œ ë°”ê¿”ì„œ ì™„ì „íˆ ìƒˆë¡œìš´ ì‘í’ˆì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”. íŠ¹íˆ í•µì‹¬ ì •ë³´ëŠ” **êµµê²Œ** í‘œì‹œí•˜ì—¬ 35-60ëŒ€ ë…ìì¸µì´ ë¹ ë¥´ê²Œ ì´í•´í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤."},
                        {"role": "user", "content": prompt}
                    ],
                    max_tokens=2000,
                    temperature=0.8
                )
                
                rewritten = response.choices[0].message.content.strip()
                # YAML ì•ˆì „ì„±ì„ ìœ„í•´ YAML êµ¬ë¶„ìë§Œ ì •ë¦¬ (ë”°ì˜´í‘œëŠ” ë³´ì¡´)
                rewritten = rewritten.replace('```', '').replace('---', 'â€”')  # YAML êµ¬ë¶„ì ë¬¸ì œ ë°©ì§€
                print(f"âœ… AI rewrite successful on attempt {attempt + 1}")
                return rewritten
                
        except Exception as e:
            print(f"âŒ AI rewrite attempt {attempt + 1} failed: {e}")
            if attempt < 2:  # ë§ˆì§€ë§‰ ì‹œë„ê°€ ì•„ë‹ˆë©´ ì¬ì‹œë„
                time.sleep(2)  # 2ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„
                continue
            else:
                print("ğŸš¨ All AI rewrite attempts failed - raising exception")
                raise Exception(f"AI rewrite failed after 3 attempts: {e}")
    
    raise Exception("AI rewrite failed - unexpected end of function")

def generate_ai_tags(title, content, existing_tags, api_key, api_type="openai"):
    """AIë¥¼ ì‚¬ìš©í•˜ì—¬ ì¶”ê°€ íƒœê·¸ ìƒì„±"""
    if not api_key:
        print("âš ï¸ No AI API key - using default tags")
        return existing_tags + ["ë‰´ìŠ¤", "ì´ìŠˆ"]
    
    for attempt in range(3):
        try:
            print(f"ğŸ·ï¸ AI tag generation attempt {attempt + 1}/3...")
            if api_type == "openai" and HAS_OPENAI:
                client = OpenAI(api_key=api_key)
                
                prompt = f"""
ê¸°ì‚¬ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ **ë…ì°½ì ì´ê³  ì°¨ë³„í™”ëœ** íƒœê·¸ 2ê°œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.
ê¸°ì¡´ íƒœê·¸ì™€ëŠ” ì™„ì „íˆ ë‹¤ë¥¸ ê´€ì ì—ì„œ ì ‘ê·¼í•´ì£¼ì„¸ìš”.

ì œëª©: {title}
ë‚´ìš©: {content[:500]}...
ê¸°ì¡´ íƒœê·¸: {', '.join(existing_tags)}

**ì°½ì˜ì  íƒœê·¸ ìƒì„± ìš”êµ¬ì‚¬í•­:**
1. ê¸°ì¡´ íƒœê·¸ì™€ ì¤‘ë³µë˜ì§€ ì•ŠëŠ” ìƒˆë¡œìš´ ê´€ì 
2. í•´ë‹¹ ì—…ê³„ì˜ ì „ë¬¸ ìš©ì–´ë‚˜ íŠ¸ë Œë“œ ë°˜ì˜
3. ê²€ìƒ‰ í‚¤ì›Œë“œë¡œ í™œìš© ê°€ëŠ¥í•œ ì‹¤ìš©ì  íƒœê·¸
4. 35~60ëŒ€ ë…ìì¸µì´ ê´€ì‹¬ ê°€ì§ˆë§Œí•œ ì£¼ì œ

**íƒœê·¸ ìŠ¤íƒ€ì¼ ì˜ˆì‹œ:**
- "ë¯¸ë˜ì „ë§", "ì—…ê³„ë™í–¥", "ì „ë¬¸ê°€ë¶„ì„", "ì‹œì¥ë³€í™”"
- "íˆ¬ìí¬ì¸íŠ¸", "ì†Œë¹„íŠ¸ë Œë“œ", "ê¸°ìˆ í˜ì‹ ", "ì •ì±…ì˜í–¥"

JSON ë°°ì—´ë¡œë§Œ ì‘ë‹µ: ["íƒœê·¸1", "íƒœê·¸2"]
"""
                
                response = client.chat.completions.create(
                    model="gpt-4.1",  # gpt-4o-mini â†’ gpt-4.1ë¡œ ë³€ê²½
                    messages=[
                        {"role": "system", "content": "ë‹¹ì‹ ì€ ì°½ì˜ì  íƒœê·¸ ìƒì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê¸°ì¡´ê³¼ëŠ” ì™„ì „íˆ ë‹¤ë¥¸ ê´€ì ì—ì„œ ë…ì°½ì ì´ê³  ì°¨ë³„í™”ëœ íƒœê·¸ë¥¼ ë§Œë“¤ì–´ë‚´ëŠ” ë§ˆì¼€íŒ… ì „ëµê°€ì…ë‹ˆë‹¤. ë…ìì˜ ê´€ì‹¬ì„ ëŒê³  ê²€ìƒ‰ íš¨ê³¼ë¥¼ ê·¹ëŒ€í™”í•˜ëŠ” í˜ì‹ ì ì¸ íƒœê·¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."},
                        {"role": "user", "content": prompt}
                    ],
                    max_tokens=100,
                    temperature=0.7
                )
                
                result = response.choices[0].message.content.strip()
                # JSON íŒŒì‹± ì‹œë„
                try:
                    new_tags = json.loads(result)
                    if isinstance(new_tags, list) and len(new_tags) >= 2:
                        print(f"âœ… AI tag generation successful on attempt {attempt + 1}")
                        return existing_tags + new_tags[:2]
                except:
                    pass
                    
        except Exception as e:
            print(f"âŒ AI tag generation attempt {attempt + 1} failed: {e}")
            if attempt < 2:
                time.sleep(1)
                continue
            else:
                print("âš ï¸ All AI tag attempts failed - using default tags")
                return existing_tags + ["ë‰´ìŠ¤", "ì´ìŠˆ"]
    
    return existing_tags + ["ë‰´ìŠ¤", "ì´ìŠˆ"]

def rewrite_title_with_ai(original_title, content, api_key, api_type="openai"):
    """AIë¥¼ ì‚¬ìš©í•˜ì—¬ ì œëª© ì¬ì‘ì„± (êµ¬ì¡° ìœ ì§€, ë‚´ìš© ë³€ê²½)"""
    if not api_key:
        print("âš ï¸ No AI API key provided, keeping original title")
        return original_title
    
    for attempt in range(3):
        try:
            print(f"ğŸ“ AI title rewrite attempt {attempt + 1}/3...")
            if api_type == "openai" and HAS_OPENAI:
                client = OpenAI(api_key=api_key)
            
            prompt = f"""
ì›ë³¸ ì œëª©ì˜ **êµ¬ì¡°ì™€ í˜•ì‹ì„ 100% ì™„ë²½í•˜ê²Œ ìœ ì§€**í•˜ë˜, ë³¸ë¬¸ ë‚´ìš©ì— ë§ê²Œ **ë”°ì˜´í‘œ ì•ˆì˜ ë‚´ìš©ë§Œ ë³€ê²½**í•´ì£¼ì„¸ìš”.

ì›ë³¸ ì œëª©: {original_title}

ë³¸ë¬¸ ë‚´ìš© (í•µì‹¬ë§Œ):
{content[:1000]}...

**ì ˆëŒ€ ì—„ìˆ˜ ìš”êµ¬ì‚¬í•­:**
1. **ë”°ì˜´í‘œ ìœ„ì¹˜ ì™„ì „ ë³´ì¡´**: "í°ë”°ì˜´í‘œ", 'ì‘ì€ë”°ì˜´í‘œ' ìœ„ì¹˜ì™€ ê°œìˆ˜ ì ˆëŒ€ ë³€ê²½ ê¸ˆì§€
2. **êµ¬ë‘ì  ì™„ì „ ë³´ì¡´**: ..(ë‘ ì ), ,(ì‰¼í‘œ), -(í•˜ì´í”ˆ) ë“± ëª¨ë“  íŠ¹ìˆ˜ê¸°í˜¸ ìœ„ì¹˜ ê·¸ëŒ€ë¡œ
3. **ë¬¸ì¥ íŒ¨í„´ ì™„ì „ ë³´ì¡´**: [ì¸ìš©ë¬¸].. [ì„¤ëª…], '[ê°ì •í‘œí˜„]' íŒ¨í„´ 100% ìœ ì§€
4. **ê¸¸ì´ íŒ¨í„´ ìœ ì§€**: ê° êµ¬ê°„(ì¸ìš©ë¬¸, ì„¤ëª…, ê°ì •í‘œí˜„)ì˜ ê¸¸ì´ë¥¼ ì›ë³¸ê³¼ ë¹„ìŠ·í•˜ê²Œ
5. **ë‚´ìš©ë§Œ êµì²´**: ë”°ì˜´í‘œ ì•ˆì˜ ë‹¨ì–´ë“¤ê³¼ ì„¤ëª… ë¶€ë¶„ë§Œ ë³¸ë¬¸ì— ë§ê²Œ ìì—°ìŠ¤ëŸ½ê²Œ ë³€ê²½

**ì™„ë²½í•œ êµ¬ì¡° ìœ ì§€ ì˜ˆì‹œ:**
ì›ë³¸: "ë¡¤ìŠ¤ë¡œì´ìŠ¤ë„ ê¸´ì¥í•˜ê² ë„¤".. ê³§ í•œêµ­ ìƒë¥™í•˜ëŠ” 500ë§ˆë ¥ ëŸ­ì…”ë¦¬ SUV, 'ê¸°ëŒ€ê° í­ë°œ'
ë³€í˜•: "ë²¤ì¸ ë„ ë‹¹í™©í•˜ê² ì–´".. ë‚´ë…„ ì¶œì‹œ ì˜ˆì •ì¸ 800ë§ˆë ¥ ì „ê¸° ì„¸ë‹¨, 'í™”ì œ ì§‘ì¤‘'

ì›ë³¸: "ì´ê±´ í˜ì‹ ì´ë‹¤".. ìƒˆë¡œìš´ AI ê¸°ìˆ  ë„ì…í•œ ìŠ¤ë§ˆíŠ¸í°, 'ê´€ì‹¬ ê¸‰ì¦'  
ë³€í˜•: "ì •ë§ ë†€ëë‹¤".. ìµœì‹  ë°°í„°ë¦¬ íƒ‘ì¬í•œ ì „ê¸° íŠ¸ëŸ­, 'ì£¼ëª© í­ë°œ'

**ì ˆëŒ€ ê¸ˆì§€ì‚¬í•­:**
- ë”°ì˜´í‘œ ê°œìˆ˜ë‚˜ ìœ„ì¹˜ ë³€ê²½
- êµ¬ë‘ì (.., , ë“±) ìœ„ì¹˜ë‚˜ ê°œìˆ˜ ë³€ê²½  
- ë¬¸ì¥ êµ¬ì¡°([ì¸ìš©].. [ì„¤ëª…], '[ê°ì •]') ë³€ê²½
- ì›ë³¸ì— ì—†ëŠ” ìƒˆë¡œìš´ êµ¬ë‘ì  ì¶”ê°€

**ëª©í‘œ: ì›ë³¸ êµ¬ì¡°ëŠ” 100% ë™ì¼í•˜ë˜, ë³¸ë¬¸ ë‚´ìš©ì— ì •í™•íˆ ë§ëŠ” í‚¤ì›Œë“œë¡œë§Œ êµì²´**

ìƒˆë¡œìš´ ì œëª©ë§Œ ì¶œë ¥ (ì„¤ëª…ì´ë‚˜ ë¶€ê°€ í…ìŠ¤íŠ¸ ì—†ì´):
"""
            
            response = client.chat.completions.create(
                model="gpt-4.1",  # gpt-4o-mini â†’ gpt-4.1ë¡œ ë³€ê²½
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ì œëª© êµ¬ì¡° ì™„ë²½ ë³´ì¡´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì›ë³¸ ì œëª©ì˜ í˜•ì‹, êµ¬ë‘ì , ë”°ì˜´í‘œ ìœ„ì¹˜ë¥¼ ì ˆëŒ€ ë³€ê²½í•˜ì§€ ì•Šê³  ì˜¤ì§ ë‚´ìš© í‚¤ì›Œë“œë§Œ êµì²´í•˜ëŠ” ê²ƒì´ íŠ¹ê¸°ì…ë‹ˆë‹¤. êµ¬ì¡° ë³´ì¡´ì„ ìµœìš°ì„ ìœ¼ë¡œ í•˜ë©°, ì›ë³¸ íŒ¨í„´ì„ 100% ìœ ì§€í•©ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=150,
                temperature=0.5  # ë” ì¼ê´€ëœ ê²°ê³¼ë¥¼ ìœ„í•´ ë‚®ì¶¤
            )
            
            new_title = response.choices[0].message.content.strip()
            # ì•ë’¤ ì‹œìŠ¤í…œ ë”°ì˜´í‘œë§Œ ì œê±° (ë‚´ìš©ì˜ ë”°ì˜´í‘œëŠ” ë³´ì¡´)
            new_title = new_title.strip('"').strip("'")
            # YAML êµ¬ë¶„ìë§Œ ì •ë¦¬ (ë”°ì˜´í‘œëŠ” ë³´ì¡´)
            new_title = new_title.replace('---', 'â€”').replace('```', '')
            print(f"âœ… AI title rewrite successful on attempt {attempt + 1}")
            print(f"ğŸ“ Title rewritten: {original_title[:50]}... â†’ {new_title[:50]}...")
            return new_title
            
        except Exception as e:
            print(f"âŒ AI title rewrite attempt {attempt + 1} failed: {e}")
            if attempt < 2:
                time.sleep(1)
                continue
            else:
                print("âš ï¸ All AI title rewrite attempts failed - using original title")
                return original_title
    
    return original_title

def extract_content_from_url(url):
    """URLì—ì„œ ê¸°ì‚¬ ë‚´ìš© ì¶”ì¶œ (ìƒˆë¡œìš´ êµ¬ì¡° ëŒ€ì‘)"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ì œëª© ì¶”ì¶œ
        title_elem = soup.find('h1', class_='entry-title')
        if not title_elem:
            return None
        title = title_elem.get_text().strip()
        
        # ë©”íƒ€ ì •ë³´ ì¶”ì¶œ - í•­ìƒ ìœ¤ì‹ ì• ë¡œ ì„¤ì • (UTF-8 ì•ˆì „)
        author = "ìœ¤ì‹ ì• "
        
        # íƒœê·¸ ì¶”ì¶œ
        tags = ["ë‰´ìŠ¤", "ì´ìŠˆ"]  # ê¸°ë³¸ íƒœê·¸
        tags_section = soup.find('span', class_='tags-links')
        if tags_section:
            tag_links = tags_section.find_all('a', rel='tag')
            for tag_link in tag_links:
                tag_text = tag_link.get_text().strip()
                if tag_text not in tags:
                    tags.append(tag_text)
        
        # ë‚´ìš© ì¶”ì¶œ
        content_elem = soup.find('div', class_='entry-content')
        if not content_elem:
            return None
        
        # ê´‘ê³  ì œê±°
        for ad in content_elem.find_all('div', class_='repoad'):
            ad.decompose()
        for ad in content_elem.find_all('ins', class_='adsbygoogle'):
            ad.decompose()
        
        # ê³µìœ  ë²„íŠ¼ ì œê±°
        for share in content_elem.find_all('ul', class_='share-list'):
            share.decompose()
        
        # ì´ë¯¸ì§€ URL ìˆ˜ì§‘ (ìˆœì„œ ë¬´ì‹œí•˜ê³  ì„ì–´ì„œ ìˆ˜ì§‘ - ì›ë³¸ ìœ„ì¹˜ì™€ ì™„ì „íˆ ë‹¤ë¥´ê²Œ)
        images = []
        for img in content_elem.find_all('img'):
            img_src = img.get('src')
            if img_src and ('wp-content/uploads' in img_src or 'reportera.b-cdn.net' in img_src):
                # ì ˆëŒ€ URLë¡œ ë³€í™˜
                if img_src.startswith('//'):
                    img_src = 'https:' + img_src
                elif img_src.startswith('/'):
                    img_src = 'https://www.reportera.co.kr' + img_src
                elif not img_src.startswith('http'):
                    img_src = 'https://www.reportera.co.kr/' + img_src
                images.append(img_src)
        
        # ì›ë³¸ ì´ë¯¸ì§€ ìˆœì„œë¥¼ ì™„ì „íˆ ì„ì–´ì„œ ë°°ì¹˜ (ì›ë³¸ê³¼ ë‹¤ë¥´ê²Œ)
        import random
        if images:
            random.shuffle(images)  # ì´ë¯¸ì§€ ìˆœì„œ ë¬´ì‘ìœ„ë¡œ ì„ê¸°
        
        # í…ìŠ¤íŠ¸ ë‚´ìš© ì¶”ì¶œ (ì´ë¯¸ì§€ ì™„ì „ ì œê±° - ì›ë³¸ ìœ„ì¹˜ ì •ë³´ ì‚­ì œ)
        paragraphs = []
        for elem in content_elem.children:
            if hasattr(elem, 'name') and elem.name:
                if elem.name in ['p', 'h1', 'h2', 'h3', 'h4', 'h5']:
                    # ì´ë¯¸ì§€ íƒœê·¸ ì™„ì „ ì œê±° (ì›ë³¸ ìœ„ì¹˜ ì •ë³´ ì‚­ì œ)
                    for img in elem.find_all('img'):
                        img.decompose()
                    
                    # í”¼ê²¨ íƒœê·¸ë„ ì œê±° (ì´ë¯¸ì§€ ìº¡ì…˜ í¬í•¨)
                    for figure in elem.find_all('figure'):
                        figure.decompose()
                        
                    # <br> íƒœê·¸ë¥¼ ì¤„ë°”ê¿ˆìœ¼ë¡œ ë³€í™˜
                    for br in elem.find_all('br'):
                        br.replace_with('\n')
                    
                    text = elem.get_text().strip()
                    # ì´ë¯¸ì§€ ê´€ë ¨ í…ìŠ¤íŠ¸ íŒ¨í„´ ì œê±°
                    text = re.sub(r'\[ì´ë¯¸ì§€.*?\]', '', text)
                    text = re.sub(r'\(ì‚¬ì§„.*?\)', '', text)
                    text = re.sub(r'ì‚¬ì§„=.*', '', text)
                    text = re.sub(r'ì´ë¯¸ì§€=.*', '', text)
                    
                    if text and not text.startswith('(adsbygoogle'):
                        if elem.name in ['h2', 'h3', 'h4', 'h5']:
                            # ì†Œì œëª©ì—ì„œ íŠ¹ìˆ˜ê¸°í˜¸ ì œê±°
                            clean_text = text.replace(':', '').replace('!', '').replace('?', '').replace('|', '').strip()
                            paragraphs.append(f"\n## {clean_text}\n")  # H2ë¡œ ë³€í™˜
                        else:
                            paragraphs.append(text)
        
        content = '\n\n'.join(paragraphs)
        
        # ìš”ì•½ë¬¸ ìƒì„± (YAML safe - ë”°ì˜´í‘œ ë³´ì¡´)
        if paragraphs:
            description = paragraphs[0][:150] + "..."
            # YAML ì•ˆì „ì„±ì„ ìœ„í•œ ê¸°ë³¸ ì •ë¦¬ (ë”°ì˜´í‘œëŠ” HTML ì—”í‹°í‹°ë¡œ ë³´ì¡´)
            description = description.replace('"', '&quot;').replace('\n', ' ').replace('\r', ' ')
            description = re.sub(r'\s+', ' ', description).strip()
        else:
            description = ""
        
        return {
            'title': title,
            'description': description,
            'content': content,
            'images': images,
            'url': url,
            'author': author,
            'tags': tags
        }
    
    except Exception as e:
        print(f"âŒ Error extracting content from {url}: {e}")
        return None

def analyze_image_text_content(image_url, api_key):
    """AI Visionìœ¼ë¡œ ì´ë¯¸ì§€ì— í…ìŠ¤íŠ¸ê°€ ìˆëŠ”ì§€ ë¶„ì„ (ë‰´ìŠ¤ ê´€ë ¨ ì´ë¯¸ì§€ ì œì™¸)"""
    if not api_key:
        return False  # API í‚¤ ì—†ìœ¼ë©´ í…ìŠ¤íŠ¸ ì—†ë‹¤ê³  ê°€ì •
    
    try:
        if HAS_OPENAI:
            client = OpenAI(api_key=api_key)
            
            # GPT-4.1ë¡œ ë¨¼ì € ì‹œë„, ì‹¤íŒ¨í•˜ë©´ gpt-4o ì‚¬ìš©
            models_to_try = ["gpt-4.1", "gpt-4o"]
            
            for model in models_to_try:
                try:
                    response = client.chat.completions.create(
                        model=model,
                        messages=[
                            {
                                "role": "user",
                                "content": [
                                    {
                                        "type": "text",
                                        "text": "ì´ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:\n1. ë‰´ìŠ¤ ê´€ë ¨: 'ì—°í•©ë‰´ìŠ¤', 'ë‰´ìŠ¤1', 'YONHAP', 'NEWS', 'ê¸°ì', 'ì œê³µ', 'ì¶œì²˜' ë“± ë‰´ìŠ¤ ê¸°ê´€ ê´€ë ¨ í…ìŠ¤íŠ¸ë‚˜ ë¡œê³ ê°€ ìˆë‚˜ìš”?\n2. ê¸°íƒ€ í…ìŠ¤íŠ¸: í•œê¸€, ì˜ì–´, ìˆ«ì ë“± ê¸°íƒ€ í…ìŠ¤íŠ¸ê°€ í¬í•¨ë˜ì–´ ìˆë‚˜ìš”?\n\në‰´ìŠ¤ ê´€ë ¨ì´ ìˆìœ¼ë©´ 'NEWS_TEXT', ê¸°íƒ€ í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ 'HAS_TEXT', í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ 'NO_TEXT'ë¡œë§Œ ë‹µë³€í•´ì£¼ì„¸ìš”."
                                    },
                                    {
                                        "type": "image_url",
                                        "image_url": {
                                            "url": image_url
                                        }
                                    }
                                ]
                            }
                        ],
                        max_tokens=20
                    )
                    
                    result = response.choices[0].message.content.strip().upper()
                    
                    # ë‰´ìŠ¤ ê´€ë ¨ í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ì œì™¸ (True ë°˜í™˜ = í…ìŠ¤íŠ¸ ìˆìŒ)
                    if "NEWS_TEXT" in result:
                        print(f"ğŸš« ë‰´ìŠ¤ ê´€ë ¨ í…ìŠ¤íŠ¸ ê°ì§€ë¡œ ì œì™¸ ({model}): {image_url[:50]}...")
                        return True  # í…ìŠ¤íŠ¸ ìˆìŒìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ ì œì™¸
                    
                    # ê¸°íƒ€ í…ìŠ¤íŠ¸ í™•ì¸
                    has_text = "HAS_TEXT" in result
                    print(f"ğŸ” ì´ë¯¸ì§€ í…ìŠ¤íŠ¸ ë¶„ì„ ({model}): {image_url[:50]}... â†’ {'í…ìŠ¤íŠ¸ ìˆìŒ' if has_text else 'í…ìŠ¤íŠ¸ ì—†ìŒ'}")
                    return has_text
                    
                except Exception as model_error:
                    if "gpt-4.1" in model:
                        print(f"âš ï¸ {model} Vision ì§€ì› ì•ˆí•¨, gpt-4oë¡œ ì¬ì‹œë„...")
                        continue  # ë‹¤ìŒ ëª¨ë¸ ì‹œë„
                    else:
                        print(f"âš ï¸ {model} ì´ë¯¸ì§€ ë¶„ì„ ì‹¤íŒ¨: {model_error}")
                        break  # gpt-4oë„ ì‹¤íŒ¨í•˜ë©´ ì¢…ë£Œ
            
    except Exception as e:
        print(f"âš ï¸ ì´ë¯¸ì§€ ë¶„ì„ ì‹¤íŒ¨: {e}")
        return False  # ë¶„ì„ ì‹¤íŒ¨ ì‹œ í…ìŠ¤íŠ¸ ì—†ë‹¤ê³  ê°€ì •
    
    return False

def generate_contextual_alt_text(paragraph_text, title, api_key):
    """ë¬¸ë§¥ì— ë§ëŠ” alt í…ìŠ¤íŠ¸ AI ìƒì„±"""
    if not api_key:
        return "ê¸°ì‚¬ ê´€ë ¨ ì´ë¯¸ì§€"
    
    try:
        if HAS_OPENAI:
            client = OpenAI(api_key=api_key)
            
            prompt = f"""
ë‹¤ìŒ ê¸°ì‚¬ì˜ ì œëª©ê³¼ ë¬¸ë‹¨ì„ ë³´ê³ , ì´ ìœ„ì¹˜ì— ë“¤ì–´ê°ˆ ì´ë¯¸ì§€ì˜ alt í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.
ì´ë¯¸ì§€ê°€ ë³¸ë¬¸ ë‚´ìš©ê³¼ ê´€ë ¨ì„±ì´ ë†’ë„ë¡ ì˜ë¯¸ ìˆëŠ” alt í…ìŠ¤íŠ¸ë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”.

ê¸°ì‚¬ ì œëª©: {title}
í•´ë‹¹ ë¬¸ë‹¨: {paragraph_text[:200]}...

ìš”êµ¬ì‚¬í•­:
1. ë³¸ë¬¸ ë‚´ìš©ê³¼ ì—°ê´€ì„± ìˆëŠ” alt í…ìŠ¤íŠ¸
2. SEOì— ë„ì›€ì´ ë˜ëŠ” í‚¤ì›Œë“œ í¬í•¨
3. 10-15ì ë‚´ì™¸ì˜ ê°„ê²°í•œ í…ìŠ¤íŠ¸
4. ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ í‘œí˜„
5. **35~60ëŒ€ ë…ìì¸µì´ ì´í•´í•˜ê¸° ì‰¬ìš´ ìš©ì–´ ì‚¬ìš©**

alt í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥í•´ì£¼ì„¸ìš”:
"""
            
            response = client.chat.completions.create(
                model="gpt-4.1",  # gpt-4o-mini â†’ gpt-4.1ë¡œ ë³€ê²½
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ SEO ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë³¸ë¬¸ ë‚´ìš©ê³¼ ì˜ ì–´ìš¸ë¦¬ëŠ” ì´ë¯¸ì§€ alt í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=50,
                temperature=0.7
            )
            
            alt_text = response.choices[0].message.content.strip()
            # ë”°ì˜´í‘œ ì œê±° ë° ì •ë¦¬
            alt_text = alt_text.strip('"').strip("'").strip()
            return alt_text if alt_text else "ê¸°ì‚¬ ê´€ë ¨ ì´ë¯¸ì§€"
    except:
        pass
    
    return "ê¸°ì‚¬ ê´€ë ¨ ì´ë¯¸ì§€"

def extract_h5_summary(content):
    """AI ìƒì„± ì½˜í…ì¸ ì—ì„œ H5 ìš”ì•½ ì¶”ì¶œí•˜ê³  ë³¸ë¬¸ì—ì„œ ì œê±°"""
    lines = content.split('\n')
    h5_summary = ""
    content_without_h5 = []
    
    for line in lines:
        if line.startswith('##### '):
            # H5 ë‚´ìš© ì¶”ì¶œ (##### ì œê±°í•˜ê³  ì €ì¥)
            h5_content = line.replace('##### ', '').strip()
            # <br> íƒœê·¸ë¥¼ ê³µë°±ìœ¼ë¡œ ë³€í™˜ (HTML íƒœê·¸ ì œê±°)
            h5_content = h5_content.replace('<br>', ' ').replace('<br/>', ' ').replace('<br />', ' ')
            # **ë³¼ë“œ ë§ˆí¬ë‹¤ìš´ ì œê±°**
            h5_content = h5_content.replace('**', '')
            # íŠ¹ìˆ˜ë¬¸ì ì œê±°
            special_chars = [':', '!', '?', '*', '#', '|', '&', '<', '>', '[', ']', '{', '}']
            for char in special_chars:
                h5_content = h5_content.replace(char, '')
            # ì—°ì†ëœ ê³µë°± ì •ë¦¬
            h5_content = re.sub(r'\s+', ' ', h5_content).strip()
            h5_summary = h5_content
        else:
            # H5ê°€ ì•„ë‹Œ ë‚´ìš©ì€ ë³¸ë¬¸ì— ìœ ì§€
            content_without_h5.append(line)
    
    # ë¹ˆ ì¤„ ì •ë¦¬
    while content_without_h5 and not content_without_h5[0].strip():
        content_without_h5.pop(0)
    
    return h5_summary, '\n'.join(content_without_h5)

def generate_section_for_image(image_url, title, existing_content, api_key):
    """ë‚¨ì€ ì´ë¯¸ì§€ë¥¼ ìœ„í•œ H2 ì†Œì œëª© + ë³¸ë¬¸ ìƒì„±"""
    if not api_key:
        return {
            'heading': "ê´€ë ¨ ì •ë³´",
            'content': "í•´ë‹¹ ë¶„ì•¼ì˜ ì¶”ê°€ì ì¸ ë™í–¥ê³¼ ë¶„ì„ ë‚´ìš©ì…ë‹ˆë‹¤."
        }
    
    try:
        if HAS_OPENAI:
            client = OpenAI(api_key=api_key)
            
            prompt = f"""
ê¸°ì‚¬ ì œëª©: {title}
ê¸°ì‚¬ ë‚´ìš© ìš”ì•½: {existing_content[:500]}...

ìœ„ ê¸°ì‚¬ì™€ ê´€ë ¨ëœ ì¶”ê°€ ì„¹ì…˜ì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”.

ìš”êµ¬ì‚¬í•­:
1. H2 ì†Œì œëª© 1ê°œ (íŠ¹ìˆ˜ê¸°í˜¸ ì—†ì´, ìì—°ìŠ¤ëŸ½ê²Œ)
2. ë³¸ë¬¸ 2-3ë¬¸ì¥ (ê¸°ì‚¬ì™€ ì—°ê´€ì„± ìˆê²Œ, **ì¤‘ìš” í‚¤ì›Œë“œëŠ” êµµê²Œ** í‘œì‹œ)
3. 35-60ëŒ€ ë…ìì¸µì—ê²Œ ìœ ìµí•œ ë‚´ìš©
4. **í•µì‹¬ ì •ë³´ëŠ” êµµê²Œ** ì²˜ë¦¬í•˜ì—¬ ì‹œì¸ì„± í–¥ìƒ

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:
{{"heading": "ì†Œì œëª©", "content": "ë³¸ë¬¸ ë‚´ìš©"}}
"""
            
            response = client.chat.completions.create(
                model="gpt-4.1",  # gpt-4o-mini â†’ gpt-4.1ë¡œ ë³€ê²½
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ê¸°ì‚¬ ì‘ì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ê¸°ì‚¬ì™€ ì—°ê´€ì„± ìˆëŠ” ì¶”ê°€ ì„¹ì…˜ì„ ë§Œë“œëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. í•µì‹¬ ì •ë³´ëŠ” **êµµê²Œ** í‘œì‹œí•˜ì—¬ 35-60ëŒ€ ë…ìì¸µì´ ë¹ ë¥´ê²Œ ì´í•´í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=200,
                temperature=0.7
            )
            
            result = response.choices[0].message.content.strip()
            try:
                import json
                section_data = json.loads(result)
                return section_data
            except:
                # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’
                return {
                    'heading': "ì¶”ê°€ ë¶„ì„",
                    'content': "ê´€ë ¨ ì—…ê³„ì˜ ë™í–¥ê³¼ ì „ë§ì— ëŒ€í•œ **ì¶”ê°€ ì •ë³´**ì…ë‹ˆë‹¤."
                }
                
    except Exception as e:
        print(f"âš ï¸ ì¶”ê°€ ì„¹ì…˜ ìƒì„± ì‹¤íŒ¨: {e}")
        return {
            'heading': "ê´€ë ¨ ë™í–¥",
            'content': "í•´ë‹¹ ë¶„ì•¼ì˜ **ìµœì‹  ë™í–¥ê³¼ ë¶„ì„**ì„ ì œê³µí•©ë‹ˆë‹¤."
        }

def insert_images_with_structure(content, cloudflare_images, title="", ai_api_key=None, category="economy"):
    """ì›ë³¸ê³¼ ì™„ì „íˆ ë‹¤ë¥¸ ìœ„ì¹˜ì— ì´ë¯¸ì§€ ë°°ì¹˜: ìš°ë¦¬ë§Œì˜ ìƒˆë¡œìš´ êµ¬ì¡°"""
    if not cloudflare_images:
        return content
    
    lines = content.split('\n')
    result_lines = []
    h5_count = 0
    h2_count = 0
    paragraph_count = 0
    
    # ì´ë¯¸ì§€ë¥¼ ì™„ì „íˆ ìƒˆë¡œìš´ ê·œì¹™ìœ¼ë¡œ ë°°ì¹˜í•˜ê¸° ìœ„í•´ ì´ë¯¸ì§€ë“¤ì„ ë‹¤ì‹œ ì„ê¸°
    import random
    shuffled_images = cloudflare_images.copy()
    random.shuffle(shuffled_images)  # ì›ë³¸ ìˆœì„œì™€ ì™„ì „íˆ ë‹¤ë¥´ê²Œ
    
    # ìš°ë¦¬ë§Œì˜ ì´ë¯¸ì§€ ë°°ì¹˜ ì „ëµ (ì¸ë„¤ì¼ì€ í…ìŠ¤íŠ¸ ì—†ëŠ” ì´ë¯¸ì§€ ìš°ì„  ì„ íƒ)
    thumbnail_image = None
    section_images = shuffled_images.copy()
    
    if shuffled_images:
        if category == "automotive":
            print("ğŸš— ìë™ì°¨ ì¹´í…Œê³ ë¦¬: ëª¨ë“  ì´ë¯¸ì§€ ì‚¬ìš© (í•„í„°ë§ ì—†ìŒ)")
            # ìë™ì°¨ëŠ” í•„í„°ë§ ì—†ì´ ì²« ë²ˆì§¸ ì´ë¯¸ì§€ ì‚¬ìš©
            thumbnail_image = section_images.pop(0)
            print(f"âœ… ì²« ë²ˆì§¸ ì´ë¯¸ì§€ë¥¼ ì¸ë„¤ì¼ë¡œ ì„ íƒ: {thumbnail_image[:50]}...")
        else:
            print("ğŸ” Economy: AI Visionìœ¼ë¡œ ë‰´ìŠ¤ í…ìŠ¤íŠ¸ ì—†ëŠ” ì¸ë„¤ì¼ ì—„ê²© ì„ ë³„ ì¤‘...")
            
            # AI Visionìœ¼ë¡œ í…ìŠ¤íŠ¸ ì—†ëŠ” ì´ë¯¸ì§€ ì°¾ê¸° (Economy ì¹´í…Œê³ ë¦¬ - ë” ë§ì´ ì²´í¬)
            text_free_images = []
            text_images = []
            
            # ìµœëŒ€ ëª¨ë“  ì´ë¯¸ì§€ë¥¼ ì²´í¬ (API ë¹„ìš©ë³´ë‹¤ í’ˆì§ˆ ìš°ì„ )
            for img_url in section_images:
                has_text = analyze_image_text_content(img_url, ai_api_key)
                if has_text:
                    text_images.append(img_url)
                    print(f"ğŸš« ë‰´ìŠ¤/í…ìŠ¤íŠ¸ ì´ë¯¸ì§€ ì œì™¸: {img_url[:50]}...")
                else:
                    text_free_images.append(img_url)
                    print(f"âœ… í…ìŠ¤íŠ¸ ì—†ëŠ” ì´ë¯¸ì§€ ë°œê²¬: {img_url[:50]}...")
                
                # í…ìŠ¤íŠ¸ ì—†ëŠ” ì´ë¯¸ì§€ë¥¼ ì°¾ìœ¼ë©´ ë°”ë¡œ ì‚¬ìš© (íš¨ìœ¨ì„±)
                if len(text_free_images) >= 1:
                    break
            
            # í…ìŠ¤íŠ¸ ì—†ëŠ” ì´ë¯¸ì§€ ìš°ì„  ì„ íƒ
            if text_free_images:
                thumbnail_image = text_free_images[0]
                section_images.remove(thumbnail_image)
                print(f"ğŸ¯ Economy ì¸ë„¤ì¼ í™•ì •: í…ìŠ¤íŠ¸ ì—†ëŠ” ì´ë¯¸ì§€ ì‚¬ìš© {thumbnail_image[:50]}...")
            else:
                print("âš ï¸ Economy: ëª¨ë“  ì´ë¯¸ì§€ì— í…ìŠ¤íŠ¸ ìˆìŒ - ì²« ë²ˆì§¸ ì´ë¯¸ì§€ ì‚¬ìš©")
                thumbnail_image = section_images.pop(0)
                print(f"ğŸ“¸ Economy ì¸ë„¤ì¼ ëŒ€ì•ˆ: {thumbnail_image[:50]}...")
    
    image_positions = {
        'thumbnail': thumbnail_image,
        'section_images': section_images
    }
    
    thumbnail_inserted = False
    section_image_index = 0
    
    for i, line in enumerate(lines):
        result_lines.append(line)
        
        # H5 ì¤„ì€ Hugo í…Œë§ˆì—ì„œ ë³„ë„ ì²˜ë¦¬í•˜ë¯€ë¡œ ì´ë¯¸ì§€ ì‚½ì…í•˜ì§€ ì•ŠìŒ
        if line.startswith('##### '):
            # H5 ì¤„ì€ ê·¸ëŒ€ë¡œ ë‘ë˜ ì´ë¯¸ì§€ëŠ” ì‚½ì…í•˜ì§€ ì•ŠìŒ (Hugoì—ì„œ ì²˜ë¦¬)
            pass
        
        # ë¬¸ë‹¨ ì¹´ìš´íŠ¸ (ì¼ë°˜ í…ìŠ¤íŠ¸) - ì´ë¯¸ì§€ ì‚½ì…í•˜ì§€ ì•ŠìŒ
        elif line.strip() and not line.startswith('#') and not line.startswith('!'):
            paragraph_count += 1
            # H2 ë’¤ì—ë§Œ ì´ë¯¸ì§€ë¥¼ ë„£ìœ¼ë¯€ë¡œ ë¬¸ë‹¨ì—ëŠ” ì´ë¯¸ì§€ ì‚½ì…í•˜ì§€ ì•ŠìŒ
        
        # H2 ì†Œì œëª© ì²˜ë¦¬ (ëª¨ë“  H2 ë’¤ì— ì´ë¯¸ì§€ ë°°ì¹˜)
        elif line.startswith('## '):
            h2_count += 1
            
            # ëª¨ë“  H2 ì†Œì œëª© ë’¤ì— ì´ë¯¸ì§€ ë°°ì¹˜ (ì¹´í…Œê³ ë¦¬ë³„ í•„í„°ë§)
            while section_image_index < len(image_positions['section_images']):
                image_url = image_positions['section_images'][section_image_index]
                section_image_index += 1
                
                # ì¹´í…Œê³ ë¦¬ë³„ ì´ë¯¸ì§€ í•„í„°ë§
                use_image = True
                if category == "economy" and ai_api_key:
                    # EconomyëŠ” ë‰´ìŠ¤ í…ìŠ¤íŠ¸ ìˆëŠ” ì´ë¯¸ì§€ ì œì™¸
                    has_text = analyze_image_text_content(image_url, ai_api_key)
                    if has_text:
                        print(f"ğŸš« Economy H2 ì„¹ì…˜: ë‰´ìŠ¤ í…ìŠ¤íŠ¸ ì´ë¯¸ì§€ ì œì™¸ {image_url[:50]}...")
                        use_image = False
                        continue  # ë‹¤ìŒ ì´ë¯¸ì§€ ì‹œë„
                # AutomotiveëŠ” ëª¨ë“  ì´ë¯¸ì§€ ì‚¬ìš© (use_image = True ìœ ì§€)
                
                if use_image:
                    if ai_api_key:
                        alt_text = generate_contextual_alt_text(line, title, ai_api_key)
                    else:
                        alt_text = line.replace('## ', '').replace('**', '').strip()
                    
                    result_lines.append("")
                    result_lines.append(f"![{alt_text}]({image_url})")
                    result_lines.append("*ì¶œì²˜: ì˜¨ë¼ì¸ ì»¤ë®¤ë‹ˆí‹°*")
                    result_lines.append("")
                    break  # ì´ë¯¸ì§€ ì‚¬ìš©í–ˆìœ¼ë¯€ë¡œ ë£¨í”„ ì¢…ë£Œ
    
    # ë‚¨ì€ ì´ë¯¸ì§€ë“¤ì„ H2 ì†Œì œëª© + ì´ë¯¸ì§€ + ë³¸ë¬¸ í˜•íƒœë¡œ ë°°ì¹˜
    remaining_images = image_positions['section_images'][section_image_index:]
    if remaining_images:
        print(f"ğŸ“ ë‚¨ì€ ì´ë¯¸ì§€ {len(remaining_images)}ê°œë¥¼ ì¶”ê°€ ì„¹ì…˜ìœ¼ë¡œ ìƒì„± ì¤‘...")
        
        # ê¸°ì¡´ ì½˜í…ì¸  ìš”ì•½ (AI ì„¹ì…˜ ìƒì„±ìš©)
        existing_content = '\n'.join(result_lines)
        
        for idx, image_url in enumerate(remaining_images):
            # ì¹´í…Œê³ ë¦¬ë³„ ì´ë¯¸ì§€ í•„í„°ë§
            use_image = True
            if category == "economy" and ai_api_key:
                # EconomyëŠ” ë‰´ìŠ¤ í…ìŠ¤íŠ¸ ìˆëŠ” ì´ë¯¸ì§€ ì œì™¸
                has_text = analyze_image_text_content(image_url, ai_api_key)
                if has_text:
                    print(f"ğŸš« Economy ì¶”ê°€ì„¹ì…˜: ë‰´ìŠ¤ í…ìŠ¤íŠ¸ ì´ë¯¸ì§€ ì œì™¸ {image_url[:50]}...")
                    continue  # ì´ ì´ë¯¸ì§€ëŠ” ê±´ë„ˆë›°ê¸°
            # AutomotiveëŠ” ëª¨ë“  ì´ë¯¸ì§€ ì‚¬ìš©
            
            # AIë¡œ ì„¹ì…˜ ìƒì„±
            section_data = generate_section_for_image(image_url, title, existing_content, ai_api_key)
            
            # H2 ì†Œì œëª© ì¶”ê°€
            result_lines.append("")
            result_lines.append(f"## {section_data['heading']}")
            result_lines.append("")
            
            # ì´ë¯¸ì§€ ì¶”ê°€
            if ai_api_key:
                alt_text = generate_contextual_alt_text(section_data['content'], title, ai_api_key)
            else:
                alt_text = section_data['heading']
            
            result_lines.append(f"![{alt_text}]({image_url})")
            result_lines.append("*ì¶œì²˜: ì˜¨ë¼ì¸ ì»¤ë®¤ë‹ˆí‹°*")
            result_lines.append("")
            
            # ë³¸ë¬¸ ì¶”ê°€
            result_lines.append(section_data['content'])
            result_lines.append("")
            
            print(f"âœ… ì¶”ê°€ ì„¹ì…˜ ìƒì„±: {section_data['heading']}")
    
    return '\n'.join(result_lines)

def validate_yaml_string(text):
    """YAMLì—ì„œ ì•ˆì „í•œ ë¬¸ìì—´ë¡œ ë³€í™˜ (HTML ì—”í‹°í‹° ì œê±°, íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬)"""
    if not text:
        return ""
    
    import html
    # HTML ì—”í‹°í‹° ë””ì½”ë”© (&quot; â†’ " ë“±)
    safe_text = html.unescape(str(text))
    
    # ê¸°ë³¸ ì •ë¦¬ (HTML êµ¬ë¶„ìë§Œ ì •ë¦¬, ë‚´ìš© ë”°ì˜´í‘œëŠ” ë³´ì¡´)
    safe_text = safe_text.replace('\n', ' ').replace('\r', ' ')
    safe_text = safe_text.replace('---', 'â€”').replace('```', '')
    
    # íŠ¹ìˆ˜ë¬¸ì ì œê±° (H5ìš©)
    special_chars = [':', '!', '?', '*', '#', '|', '&', '<', '>', '[', ']', '{', '}']
    for char in special_chars:
        safe_text = safe_text.replace(char, '')
    
    # ì—°ì†ëœ ê³µë°± ì •ë¦¬
    safe_text = re.sub(r'\s+', ' ', safe_text).strip()
    
    # ê¸¸ì´ ì œí•œ
    if len(safe_text) > 200:
        safe_text = safe_text[:200] + "..."
    
    return safe_text

def create_markdown_file(article_data, output_dir, article_index=0, general_count=0, total_count=0, cloudflare_account_id=None, cloudflare_api_token=None, ai_api_key=None):
    """ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ìƒì„± (AI ì¬ì‘ì„± ë° ì´ë¯¸ì§€ ì²˜ë¦¬ í¬í•¨)"""
    # ğŸ›¡ï¸ ê°•í™”ëœ ë‹¤ë‹¨ê³„ ì¤‘ë³µ ì²´í¬
    article_hash = get_article_hash(article_data['title'], article_data['url'])
    
    # 1. URL ê¸°ë°˜ DB ì²´í¬ (ìµœìš°ì„  - ê°€ì¥ ë¹ ë¥´ê³  í™•ì‹¤)
    db_path = 'processed_articles.db'
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    cursor.execute('SELECT COUNT(*) FROM processed_articles WHERE url = ?', (article_data['url'],))
    url_exists = cursor.fetchone()[0] > 0
    conn.close()
    
    if url_exists:
        print(f"â­ï¸ Skipping duplicate article (URL in DB): {article_data['title'][:50]}...")
        return False
    
    # 2. ì „ì²´ DB ê¸°ë°˜ ì¤‘ë³µ ì²´í¬ (í•´ì‹œ í¬í•¨)
    if is_article_processed(article_data['url'], article_data['title'], article_hash):
        print(f"â­ï¸ Skipping duplicate article (Hash in DB): {article_data['title'][:50]}...")
        return False
    
    # 3. íŒŒì¼ ê¸°ë°˜ ì¤‘ë³µ ì²´í¬ (ì•ˆì „ì¥ì¹˜ - íŒŒì¼ì‹œìŠ¤í…œê³¼ DB ë¶ˆì¼ì¹˜ ëŒ€ë¹„)
    if check_existing_articles(output_dir, article_hash, article_data['title'], article_data['url']):
        print(f"â­ï¸ Skipping duplicate article (Found in Files): {article_data['title'][:50]}...")
        # DBì—ë„ ê¸°ë¡í•˜ì—¬ ë‹¤ìŒë²ˆì—” ë” ë¹ ë¥´ê²Œ ìŠ¤í‚µ
        mark_article_processed(article_data['url'], article_data['title'], article_hash)
        return False
    
    print(f"ğŸ¤– Processing NEW article with AI: {article_data['title'][:50]}...")
    
    # AIë¡œ ì œëª© ì¬ì‘ì„± (êµ¬ì¡° ìœ ì§€, ë‚´ìš© ë³€ê²½)
    new_title = rewrite_title_with_ai(
        article_data['title'],
        article_data['content'],
        ai_api_key
    )
    
    # AI ì œëª© ì¬ì‘ì„± ì‹¤íŒ¨ ì‹œ ê¸°ì‚¬ ìƒì„± ê±´ë„ˆë›°ê¸°
    if not new_title or new_title == article_data['title']:
        print(f"âš ï¸ AI title rewrite failed, skipping article: {article_data['title'][:50]}...")
        return False
    
    # AIë¡œ ê¸°ì‚¬ ì¬ì‘ì„±
    rewritten_content = rewrite_with_ai(
        article_data['content'], 
        new_title,  # ìƒˆë¡œìš´ ì œëª© ì‚¬ìš©
        ai_api_key
    )
    
    # AI ê¸°ì‚¬ ì¬ì‘ì„± ì‹¤íŒ¨ ì‹œ ê¸°ì‚¬ ìƒì„± ê±´ë„ˆë›°ê¸°
    if not rewritten_content or rewritten_content == article_data['content']:
        print(f"âš ï¸ AI content rewrite failed, skipping article: {new_title[:50]}...")
        return False
    
    # H5 ìš”ì•½ì„ YAMLìš©ìœ¼ë¡œ ë¶„ë¦¬
    h5_summary, content_without_h5 = extract_h5_summary(rewritten_content)
    print(f"ğŸ“ H5 ìš”ì•½ ì¶”ì¶œ: {h5_summary[:50]}..." if h5_summary else "âš ï¸ H5 ìš”ì•½ ì—†ìŒ")
    
    # AIë¡œ íƒœê·¸ ì¶”ê°€ ìƒì„±
    enhanced_tags = generate_ai_tags(
        new_title,  # ìƒˆë¡œìš´ ì œëª© ì‚¬ìš©
        article_data['content'],
        article_data['tags'],
        ai_api_key
    )
    
    # Cloudflareì— ì´ë¯¸ì§€ ì—…ë¡œë“œ (ì›ë³¸ ìˆœì„œì™€ ì™„ì „íˆ ë‹¤ë¥´ê²Œ - ì—­ìˆœìœ¼ë¡œ)
    cloudflare_images = []
    if cloudflare_api_token and cloudflare_account_id and article_data['images']:
        # ì›ë³¸ê³¼ ë‹¤ë¥´ê²Œ ì—­ìˆœìœ¼ë¡œ ì—…ë¡œë“œí•˜ì—¬ ìœ„ì¹˜ ì™„ì „ ë³€ê²½
        reversed_images = list(reversed(article_data['images'][:5]))  # ì—­ìˆœ + ìµœëŒ€ 5ê°œ
        print(f"ğŸ“¸ Uploading {len(reversed_images)} images to Cloudflare (in reverse order)...")
        
        for img_url in reversed_images:
            cf_url = upload_to_cloudflare_images(img_url, cloudflare_api_token, cloudflare_account_id)
            if cf_url:  # ì„±ê³µí•œ ê²½ìš°ë§Œ ì¶”ê°€ (ì›ë³¸ ìˆœì„œì™€ ì™„ì „íˆ ë‹¤ë¦„)
                cloudflare_images.append(cf_url)
            time.sleep(1)  # API ì œí•œ ê³ ë ¤
    
    # ì¹´í…Œê³ ë¦¬ ë¶„ë°°: ì²˜ìŒ 20%ëŠ” ì¼ë°˜ì‚¬ì´íŠ¸ë§µ(automotive), ë‚˜ë¨¸ì§€ 80%ëŠ” ë‰´ìŠ¤ì‚¬ì´íŠ¸ë§µ(economy)
    if article_index < general_count:
        category = 'automotive'  # ì¼ë°˜ì‚¬ì´íŠ¸ë§µìœ¼ë¡œ ë°°ì¹˜
        print(f"ğŸ“‹ ì¼ë°˜ì‚¬ì´íŠ¸ë§µ ë°°ì¹˜ ({article_index + 1}/{general_count})")
    else:
        category = 'economy'  # ë‰´ìŠ¤ì‚¬ì´íŠ¸ë§µìœ¼ë¡œ ë°°ì¹˜
        print(f"ğŸ“° ë‰´ìŠ¤ì‚¬ì´íŠ¸ë§µ ë°°ì¹˜ ({article_index + 1 - general_count}/{total_count - general_count})")
    
    # ì´ë¯¸ì§€ë¥¼ ì›ë³¸ê³¼ ì™„ì „íˆ ë‹¤ë¥¸ ìœ„ì¹˜ì— ë°°ì¹˜ (H5 ì œê±°ëœ ë³¸ë¬¸ ì‚¬ìš©, ì¹´í…Œê³ ë¦¬ë³„ í•„í„°ë§)
    final_content = insert_images_with_structure(content_without_h5, cloudflare_images, new_title, ai_api_key, category)
    
    # URL ìŠ¬ëŸ¬ê·¸ ìƒì„± (ìƒˆ ì œëª© ê¸°ë°˜)
    title_slug = create_url_slug(new_title)
    
    # ì¹´í…Œê³ ë¦¬ë³„ ë””ë ‰í† ë¦¬ ìƒì„±
    category_dir = os.path.join(output_dir, category)
    os.makedirs(category_dir, exist_ok=True)
    
    # íŒŒì¼ëª… ìƒì„±: ì¹´í…Œê³ ë¦¬/ì œëª©-ì˜ë¬¸.md
    filename = f"{title_slug}.md"
    filepath = os.path.join(category_dir, filename)
    
    # íŒŒì¼ëª… ì¤‘ë³µ ë°©ì§€
    counter = 1
    while os.path.exists(filepath):
        filename = f"{title_slug}-{counter}.md"
        filepath = os.path.join(category_dir, filename)
        counter += 1
    
    # í˜„ì¬ ë‚ ì§œ (í•œêµ­ ì‹œê°„ëŒ€)
    kst = timezone(timedelta(hours=9))
    current_date = datetime.now(kst).strftime("%Y-%m-%dT%H:%M:%S+09:00")
    
    # YAML-safe description ìƒì„±
    safe_description = validate_yaml_string(article_data['description'])
    
    # YAML-safe title ìƒì„±  
    safe_title = validate_yaml_string(new_title)
    
    # ë§ˆí¬ë‹¤ìš´ ìƒì„± (UTF-8 ì•ˆì „í•œ author í•„ë“œ)
    safe_author = "ìœ¤ì‹ ì• "  # í•˜ë“œì½”ë”©ìœ¼ë¡œ ì¸ì½”ë”© ë¬¸ì œ ë°©ì§€
    
    # ë‚ ì§œ í¬ë§·íŒ… (í•œêµ­ ì‹œê°„ëŒ€)
    kst_date = datetime.now(kst)
    formatted_date = kst_date.strftime("%Yë…„ %mì›” %dì¼ %H:%M")
    
    # ì¹´í…Œê³ ë¦¬ í•œê¸€ëª…
    category_korean = "Economy" if category == "economy" else "Automotive"
    
    # YAML ì•ˆì „í•œ ì œëª© ì²˜ë¦¬ (ë”°ì˜´í‘œ ì´ìŠ¤ì¼€ì´í”„)
    yaml_safe_title = new_title.replace('"', '\\"') if new_title else safe_title
    yaml_safe_h5 = h5_summary.replace('"', '\\"') if h5_summary else ""
    
    markdown_content = f"""---
title: "{yaml_safe_title}"
description: "{safe_description}"
date: {current_date}
author: "{safe_author}"
categories: ["{category}"]
tags: {json.dumps(enhanced_tags, ensure_ascii=False)}
hash: {article_hash}
source_url: "{article_data['url']}"
url: "/{category}/{title_slug}/"
h5_summary: "{yaml_safe_h5}"
"""
    
    # Cloudflare Imagesë§Œ ì‚¬ìš© (ì›ë³¸ ì´ë¯¸ì§€ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
    if cloudflare_images:
        thumbnail_image = cloudflare_images[0]
        markdown_content += f'images: {json.dumps(cloudflare_images, ensure_ascii=False)}\n'
        markdown_content += f'thumbnail: "{thumbnail_image}"\n'
        markdown_content += f'image: "{thumbnail_image}"\n'  # Open Graphìš©
        markdown_content += f'featured_image: "{thumbnail_image}"\n'  # í…Œë§ˆë³„ í˜¸í™˜ì„±
        markdown_content += f'image_width: 1200\n'  # Google Discover ìµœì í™”
        markdown_content += f'image_height: 630\n'  # Google Discover ìµœì í™”
    
    # SEO ìµœì í™” ì¶”ê°€ í•„ë“œ
    markdown_content += f'slug: "{title_slug}"\n'
    markdown_content += f'type: "post"\n'
    markdown_content += f'layout: "single"\n'
    markdown_content += f'news_keywords: "{", ".join(enhanced_tags[:5])}"\n'  # Google News ìµœì í™”
    markdown_content += f'robots: "index, follow"\n'  # ê²€ìƒ‰ì—”ì§„ í¬ë¡¤ë§ í—ˆìš©
    
    markdown_content += f"""draft: false
---

{final_content}
"""
    
    # íŒŒì¼ ì €ì¥
    try:
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
        
        # ğŸ“ DBì— ì²˜ë¦¬ ì™„ë£Œ ê¸°ë¡ (íŒŒì¼ ìƒì„± ì„±ê³µ í›„ì—ë§Œ)
        mark_article_processed(article_data['url'], article_data['title'], article_hash)
        
        print(f"âœ… Created: {category}/{os.path.basename(filepath)}")
        
        # Hugo ì‚¬ì´íŠ¸ ì¬ë¹Œë“œ (ë©”ì¸í˜ì´ì§€ì— ìƒˆ ê¸°ì‚¬ ë°˜ì˜)
        rebuild_hugo_site()
        
        return True
        
    except Exception as e:
        print(f"âŒ Failed to create file {filepath}: {e}")
        return False

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # í™˜ê²½ë³€ìˆ˜ì—ì„œ ì„¤ì • ì½ê¸° (ìƒˆë¡œìš´ Cloudflare ì„¤ì •)
    sitemap_url = get_env_var('SITEMAP_URL', 'https://www.reportera.co.kr/news-sitemap.xml')
    cloudflare_account_id = get_env_var('CLOUDFLARE_ACCOUNT_ID', '5778a7b9867a82c2c6ad6d104d5ebb6d')
    cloudflare_api_token = get_env_var('CLOUDFLARE_API_TOKEN', 'XLz-RMI1mpfrTEqLnKylT6t8tJEO7Drcx0zopcGf')
    ai_api_key = get_env_var('OPENAI_API_KEY')
    
    # ì²˜ë¦¬ëœ ê¸°ì‚¬ DB ì´ˆê¸°í™”
    init_processed_db()
    
    if len(sys.argv) > 1:
        sitemap_url = sys.argv[1]
    
    print(f"ğŸš€ Starting AI-powered scraper...")
    print(f"ğŸ“¥ Sitemap: {sitemap_url}")
    print(f"ğŸ¤– AI Rewrite: {'âœ…' if ai_api_key else 'âŒ'}")
    print(f"â˜ï¸ Cloudflare Images: {'âœ…' if cloudflare_api_token else 'âŒ'}")
    
    # ì‚¬ì´íŠ¸ë§µ ë‹¤ìš´ë¡œë“œ
    try:
        response = requests.get(sitemap_url)
        response.raise_for_status()
        sitemap_content = response.text
        print(f"âœ… Downloaded sitemap: {len(sitemap_content):,} bytes")
    except Exception as e:
        print(f"âŒ Error downloading sitemap: {e}")
        sys.exit(1)
    
    # URL ì¶”ì¶œ (ë‰´ìŠ¤ ì‚¬ì´íŠ¸ë§µ)
    news_urls = []
    try:
        root = ET.fromstring(sitemap_content)
        # news sitemap ë„¤ì„ìŠ¤í˜ì´ìŠ¤
        namespaces = {
            '': 'http://www.sitemaps.org/schemas/sitemap/0.9',
            'news': 'http://www.google.com/schemas/sitemap-news/0.9'
        }
        
        for url_elem in root.findall('.//url', namespaces):
            loc_elem = url_elem.find('loc', namespaces)
            if loc_elem is not None:
                url = loc_elem.text
                if url and url.startswith('https://www.reportera.co.kr/'):
                    news_urls.append(url)
                    
    except Exception as e:
        print(f"âš ï¸ Error parsing XML: {e}")
        # ëŒ€ì•ˆ íŒŒì‹±
        lines = sitemap_content.split('\n')
        for line in lines:
            if '<loc>' in line and '</loc>' in line:
                start = line.find('<loc>') + 5
                end = line.find('</loc>')
                if start > 4 and end > start:
                    url = line[start:end]
                    if url.startswith('https://www.reportera.co.kr/'):
                        news_urls.append(url)
    
    # ì›ë³¸ ìˆ˜ì§‘ì€ 100% ë‰´ìŠ¤ ì‚¬ì´íŠ¸ë§µì—ì„œë§Œ
    urls = news_urls  # ë‰´ìŠ¤ URLë§Œ ì‚¬ìš©
    import random
    random.shuffle(urls)  # ìˆœì„œ ì„ê¸°
    
    # ë‚´ ì‚¬ì´íŠ¸ ë°°ì¹˜ ê³„íš: 20% ì¼ë°˜ì‚¬ì´íŠ¸ë§µ + 80% ë‰´ìŠ¤ì‚¬ì´íŠ¸ë§µ
    total_articles = len(urls)
    general_count = min(200, int(total_articles * 0.2))  # ì¼ë°˜ì‚¬ì´íŠ¸ë§µ ìµœëŒ€ 200ê°œ
    news_count = total_articles - general_count  # ë‚˜ë¨¸ì§€ëŠ” ë‰´ìŠ¤ì‚¬ì´íŠ¸ë§µ
    
    print(f"ğŸ“Š ì›ë³¸ ìˆ˜ì§‘ ë° ë°°ì¹˜ ê³„íš:")
    print(f"   ğŸ—ï¸ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ë§µì—ì„œ ìˆ˜ì§‘: {len(news_urls)}ê°œ")
    print(f"   ğŸ¯ ì´ ì²˜ë¦¬ ëŒ€ìƒ: {len(urls)}ê°œ")
    print(f"   ğŸ“‹ ì¼ë°˜ì‚¬ì´íŠ¸ë§µ ë°°ì¹˜: {general_count}ê°œ (20%)")
    print(f"   ğŸ“° ë‰´ìŠ¤ì‚¬ì´íŠ¸ë§µ ë°°ì¹˜: {news_count}ê°œ (80%)")
    
    # ğŸ”¥ ë¶„ë°°ëœ ê¸°ì‚¬ ì²˜ë¦¬
    print(f"ğŸ” ìŠ¤ë§ˆíŠ¸ ì‚¬ì´íŠ¸ë§µ ë¶„ë°° ì™„ë£Œ - {len(urls)}ê°œ URL ì²˜ë¦¬ ì‹œì‘")
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬
    output_dir = 'content'
    os.makedirs(output_dir, exist_ok=True)
    
    # ğŸ“Š ì²˜ë¦¬ ì „ ì¤‘ë³µ ì²´í¬ í†µê³„
    duplicate_count = 0
    db_path = 'processed_articles.db'
    
    if os.path.exists(db_path):
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        for url in urls:
            cursor.execute('SELECT COUNT(*) FROM processed_articles WHERE url = ?', (url,))
            if cursor.fetchone()[0] > 0:
                duplicate_count += 1
        
        conn.close()
    
    print(f"ğŸ“ˆ Processing Statistics:")
    print(f"   ğŸ”— Total URLs: {len(urls)}")
    print(f"   ğŸ”„ Already processed: {duplicate_count}")
    print(f"   ğŸ†• New to process: {len(urls) - duplicate_count}")
    
    # ì²˜ë¦¬ í†µê³„
    processed = 0
    skipped = 0
    failed = 0
    
    for i, url in enumerate(urls):
        print(f"\nğŸ“„ [{i+1}/{len(urls)}] Processing: {url.split('/')[-2:]}")
        
        # ğŸ›¡ï¸ URL ê¸°ë°˜ ì‚¬ì „ ì¤‘ë³µ ì²´í¬ (ë¹ ë¥¸ ìŠ¤í‚µ)
        if os.path.exists(db_path):
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            cursor.execute('SELECT COUNT(*) FROM processed_articles WHERE url = ?', (url,))
            is_processed = cursor.fetchone()[0] > 0
            conn.close()
            
            if is_processed:
                print(f"â­ï¸ Skipping already processed URL: {url}")
                skipped += 1
                continue
        
        article_data = extract_content_from_url(url)
        
        if article_data:
            if create_markdown_file(
                article_data, 
                output_dir,
                i,  # article_index
                general_count,  # general_count
                len(urls),  # total_count
                cloudflare_account_id,
                cloudflare_api_token,
                ai_api_key
            ):
                processed += 1
                print(f"ğŸ¯ Progress: {processed} processed, {skipped} skipped, {failed} failed")
            else:
                skipped += 1
        else:
            failed += 1
            print(f"âŒ Failed to extract content from: {url}")
        
        # API ì œí•œ ê³ ë ¤ ëŒ€ê¸° (ì²˜ë¦¬ëŸ‰ì— ë”°ë¼ ì¡°ì •)
        if processed > 0 and processed % 10 == 0:
            print(f"â¸ï¸ Processed {processed} articles, taking a short break...")
            time.sleep(5)  # 10ê°œë§ˆë‹¤ 5ì´ˆ ëŒ€ê¸°
        else:
            time.sleep(random.uniform(1, 2))
    
    print(f"\nğŸ“Š Final Processing Summary:")
    print(f"âœ… Successfully Processed: {processed}")
    print(f"â­ï¸ Skipped (Duplicates): {skipped}")
    print(f"âŒ Failed: {failed}")
    print(f"ğŸ“ˆ Total URLs Checked: {len(urls)}")
    
    if processed > 0:
        print(f"ğŸ‰ Successfully created {processed} new AI-rewritten articles!")
        print(f"ğŸ’¾ Database updated with {processed + skipped} processed URLs")
    else:
        print("â„¹ï¸ No new articles were created - all URLs already processed or failed")
    
    # ğŸ“Š DB ìƒíƒœ í™•ì¸
    try:
        db_path = 'processed_articles.db'
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        cursor.execute('SELECT COUNT(*) FROM processed_articles')
        total_processed = cursor.fetchone()[0]
        conn.close()
        print(f"ğŸ—„ï¸ Total articles in database: {total_processed}")
    except Exception as e:
        print(f"âš ï¸ Could not check database: {e}")
    
    # ğŸ“§ ì´ë©”ì¼ ë³´ê³ ì„œ ë°œì†¡
    print(f"\nğŸ“§ Sending email report...")
    try:
        # send_email.pyì˜ í•¨ìˆ˜ import ë° ì‹¤í–‰
        import importlib.util
        
        # send_email.py ëª¨ë“ˆ ë™ì  ë¡œë“œ
        spec = importlib.util.spec_from_file_location("send_email", "send_email.py")
        if spec and spec.loader:
            send_email_module = importlib.util.module_from_spec(spec)
            sys.modules["send_email"] = send_email_module
            spec.loader.exec_module(send_email_module)
            
            # ì´ë©”ì¼ ë³´ê³ ì„œ ë°œì†¡
            email_success = send_email_module.send_report_email()
            if email_success:
                print("âœ… Email report sent successfully!")
            else:
                print("âš ï¸ Email report failed to send")
        else:
            print("âš ï¸ Could not load send_email.py module")
            
    except Exception as e:
        print(f"âš ï¸ Email sending error: {e}")
        print("ğŸ“§ Skipping email report...")

if __name__ == "__main__":
    main() 