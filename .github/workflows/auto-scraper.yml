name: Auto Content Scraper

on:
  # n8n ì›¹í›…ìœ¼ë¡œ íŠ¸ë¦¬ê±°
  repository_dispatch:
    types: [scrape-content]
  
  # ìˆ˜ë™ ì‹¤í–‰ë„ ê°€ëŠ¥
  workflow_dispatch:
    inputs:
      sitemap_url:
        description: 'Sitemap URL to scrape'
        required: false
        default: 'https://www.reportera.co.kr/sitemap.xml'

permissions:
  contents: write

jobs:
  scrape-and-deploy:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.PAT_TOKEN || secrets.GITHUB_TOKEN }}
          
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          
      - name: Install Python dependencies
        run: |
          pip install requests beautifulsoup4 openai unidecode
          
      - name: Create content directory
        run: |
          mkdir -p content
          
      - name: Run AI scraper
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
        run: |
          echo "ğŸš€ Starting AI-powered scraper"
          python ai_scraper.py
          
      - name: Check for new content
        id: check_changes
        run: |
          git add .
          if git diff --staged --quiet; then
            echo "has_changes=false" >> $GITHUB_OUTPUT
            echo "No new content found"
          else
            echo "has_changes=true" >> $GITHUB_OUTPUT
            echo "New content detected"
          fi
          
      - name: Commit and push changes
        if: steps.check_changes.outputs.has_changes == 'true'
        run: |
          git config --local user.email "hangil9910@gmail.com"
          git config --local user.name "gkstn15234"
          
          # ìŠ¤í¬ë˜í•‘ëœ íŒŒì¼ ìˆ˜ ê³„ì‚°
          NEW_FILES=$(git diff --staged --name-only | grep -E '\.md$' | wc -l)
          
          git commit -m "ğŸ¤– Auto-scraped $NEW_FILES new articles $(date '+%Y-%m-%d %H:%M')"
          git push
          
      - name: Notify completion
        if: steps.check_changes.outputs.has_changes == 'true'
        run: |
          echo "âœ… Content scraping completed and pushed to repository"
          echo "ğŸš€ Cloudflare Pages will automatically deploy the changes"
          
      - name: No changes notification
        if: steps.check_changes.outputs.has_changes == 'false'
        run: |
          echo "â„¹ï¸ No new content found to scrape" 